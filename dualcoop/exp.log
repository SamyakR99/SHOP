nohup: ignoring input
 -------------------- Building Dataset ----------------------
DATASET.ROOT = /home/samyakr2/SHOP/DualCoOp/data/VOCdevkit/VOC2007
data_split = trainval
PARTIAL_PORTION= 1.000000
INPUT.SIZE = 448
 -------------------- Building Dataset ----------------------
DATASET.ROOT = /home/samyakr2/SHOP/DualCoOp/data/VOCdevkit/VOC2007
data_split = test
PARTIAL_PORTION= 1.000000
INPUT.SIZE = 448
 -------------------- Building Dataset ----------------------
DATASET.ROOT = /home/samyakr2/SHOP/DualCoOp/data/VOCdevkit/VOC2007
data_split = test
PARTIAL_PORTION= 1.000000
INPUT.SIZE = 448
Loading CLIP (backbone: RN101)
Skip visual.attnpool.linear_projection.weight
Skip visual.attnpool.linear_projection.bias
Building dualcoop
Initializing class-specific contexts
Initial positive context: "X X X X X X X X X X X X X X X X"
Initial negative  context: "X X X X X X X X X X X X X X X X"
Number of positive context words (tokens): 16
Number of negative context words (tokens): 16
Freeze the backbone weights
Freeze the attn weights
image_encoder.attnpool.positional_embedding
image_encoder.attnpool.k_proj.weight
image_encoder.attnpool.k_proj.bias
image_encoder.attnpool.q_proj.weight
image_encoder.attnpool.q_proj.bias
image_encoder.attnpool.v_proj.weight
image_encoder.attnpool.v_proj.bias
image_encoder.attnpool.c_proj.weight
image_encoder.attnpool.c_proj.bias
image_encoder.attnpool.linear_projection.weight
image_encoder.attnpool.linear_projection.bias
Multiple GPUs detected (n_gpus=2), use all of them!
num of params in prompt learner:  2
train.py --config_file configs/models/rn101_ep50.yaml --datadir /home/samyakr2/SHOP/DualCoOp/data/VOCdevkit/VOC2007 --dataset_config_file configs/datasets/voc2007.yaml --input_size 448 --lr 0.001 --loss_w 0.03 -pp 1 --csc
Namespace(prefix='', resume=None, pretrained=None, auto_resume=False, datadir='/home/samyakr2/SHOP/DualCoOp/data/VOCdevkit/VOC2007', input_size=448, train_input_size=None, num_train_cls=100, test_input_size=None, thre=0.5, single_prompt='pos', output_dir='', print_freq=100, val_freq_in_epoch=-1, evaluate=False, config_file='configs/models/rn101_ep50.yaml', dataset_config_file='configs/datasets/voc2007.yaml', positive_prompt=None, negative_prompt=None, n_ctx_pos=None, n_ctx_neg=None, lr=0.001, loss_w=0.03, csc=True, logit_scale=100.0, gamma_neg=2.0, gamma_pos=1.0, portion=1.0, partial_portion=1.0, mask_file=None, train_batch_size=None, stop_epochs=None, max_epochs=None, finetune=False, finetune_backbone=False, finetune_attn=False, finetune_text=False, base_lr_mult=None, backbone_lr_mult=None, text_lr_mult=None, attn_lr_mult=None, val_every_n_epochs=1, warmup_epochs=1, top_k=3)
DataParallel(
  (module): DualCoop(
    (prompt_learner): MLCPromptLearner()
    (image_encoder): ModifiedResNet_conv_proj(
      (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
      (relu): ReLU(inplace=True)
      (layer1): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (-1): AvgPool2d(kernel_size=1, stride=1, padding=0)
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (layer2): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (layer3): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (6): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (7): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (8): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (9): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (10): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (11): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (12): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (13): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (14): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (15): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (16): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (17): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (18): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (19): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (20): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (21): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (22): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (layer4): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (attnpool): AttentionConv(
        (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
        (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
        (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
        (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        (linear_projection): Linear(in_features=2048, out_features=512, bias=True)
      )
    )
    (text_encoder): TextEncoder(
      (transformer): Transformer(
        (resblocks): Sequential(
          (0): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (1): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (2): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (3): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (4): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (5): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (6): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (7): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (8): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (9): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (10): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (11): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
  )
)
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 3
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
    SHUFFLE: False
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    PARTIAL_PORTION: 1.0
    PORTION: 1.0
    SAMPLER: RandomSampler
    SHUFFLE: True
  VAL:
    BATCH_SIZE: 100
    SHUFFLE: False
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  MASK_FILE: None
  NAME: voc2007
  NUM_LABELED: -1
  NUM_SHOTS: -1
  ROOT: /home/samyakr2/SHOP/DualCoOp/data/VOCdevkit/VOC2007
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  TARGET_DOMAINS: ()
  TEST_GZSL_SPLIT: 
  TEST_SPLIT: test
  TRAIN_SPLIT: trainval
  VAL_GZSL_SPLIT: 
  VAL_PERCENT: 0.1
  VAL_SPLIT: test
  ZS_TEST: 
  ZS_TEST_UNSEEN: 
  ZS_TRAIN: 
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (448, 448)
  TEST:
    SIZE: (448, 448)
  TRAIN:
    SIZE: (448, 448)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MLCCLIP:
  FLOAT: False
  NEGATIVE_PROMPT: 
  POSITIVE_PROMPT: 
MODEL:
  BACKBONE:
    NAME: RN101
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  ATTN_LR_MULT: 0.1
  BACKBONE_LR_MULT: 0.1
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.001
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: ./output
RESUME: 
SEED: -1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 100
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COOP_MLC:
    ASL_GAMMA_NEG: 2.0
    ASL_GAMMA_POS: 1.0
    CSC: True
    LS: 100.0
    NEGATIVE_PROMPT_INIT: 
    N_CTX_NEG: 16
    N_CTX_POS: 16
    POSITIVE_PROMPT_INIT: 
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FINETUNE: False
  FINETUNE_ATTN: False
  FINETUNE_BACKBONE: False
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: 
  RESNET_IMAGENET:
    DEPTH: 50
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
/home/samyakr2/anaconda3/lib/python3.11/site-packages/torch/_tensor.py:836: UserWarning: non-inplace resize is deprecated
  warnings.warn("non-inplace resize is deprecated")
Train: [0/157]	Time 3.454 (3.454)	Loss 12.17 (12.17)	mAP 15.51 (15.51)
Train: [100/157]	Time 0.451 (0.478)	Loss 1.44 (2.14)	mAP 22.04 (16.14)
Train: [1/50]	Time 0.470	Loss 1.91 	mAP 18.20
Test: [0/50]	Time 1.905 (1.905)	Precision 21.33 (21.33)	Recall 44.20 (44.20) 	 P_C 15.18 	 R_C 27.65 	 F_C 14.61 	 P_O 21.33 	 R_O 44.20 	 F_O 28.77
Test: [1/50]	  P_C 14.34 	 R_C 28.91 	 F_C 17.67 	 P_O 25.26 	 R_O 49.94 	 F_O 33.55 	 mAP 14.28
Train: [0/157]	Time 1.077 (1.077)	Loss 1.49 (1.49)	mAP 30.48 (30.48)
Train: [100/157]	Time 0.465 (0.471)	Loss 0.77 (1.12)	mAP 46.81 (50.49)
Train: [2/50]	Time 0.468	Loss 1.02 	mAP 54.07
Test: [0/50]	Time 1.559 (1.559)	Precision 54.95 (54.95)	Recall 80.43 (80.43) 	 P_C 53.04 	 R_C 74.99 	 F_C 56.63 	 P_O 54.95 	 R_O 80.43 	 F_O 65.29
Test: [2/50]	  P_C 54.91 	 R_C 74.76 	 F_C 60.48 	 P_O 55.78 	 R_O 80.34 	 F_O 65.84 	 mAP 70.47
Train: [0/157]	Time 1.086 (1.086)	Loss 0.91 (0.91)	mAP 49.62 (49.62)
Train: [100/157]	Time 0.460 (0.471)	Loss 0.79 (0.77)	mAP 60.04 (62.23)
Train: [3/50]	Time 0.470	Loss 0.75 	mAP 63.31
Test: [0/50]	Time 1.568 (1.568)	Precision 54.95 (54.95)	Recall 88.41 (88.41) 	 P_C 61.63 	 R_C 83.02 	 F_C 66.53 	 P_O 54.95 	 R_O 88.41 	 F_O 67.78
Test: [3/50]	  P_C 60.45 	 R_C 79.94 	 F_C 67.36 	 P_O 57.40 	 R_O 85.87 	 F_O 68.80 	 mAP 76.54
Train: [0/157]	Time 0.979 (0.979)	Loss 0.67 (0.67)	mAP 68.67 (68.67)
Train: [100/157]	Time 0.458 (0.475)	Loss 0.61 (0.68)	mAP 76.79 (65.14)
Train: [4/50]	Time 0.473	Loss 0.68 	mAP 65.85
Test: [0/50]	Time 1.660 (1.660)	Precision 61.26 (61.26)	Recall 84.78 (84.78) 	 P_C 64.14 	 R_C 82.36 	 F_C 68.20 	 P_O 61.26 	 R_O 84.78 	 F_O 71.12
Test: [4/50]	  P_C 64.78 	 R_C 79.93 	 F_C 69.77 	 P_O 65.20 	 R_O 83.42 	 F_O 73.19 	 mAP 79.16
Train: [0/157]	Time 1.080 (1.080)	Loss 0.68 (0.68)	mAP 68.35 (68.35)
Train: [100/157]	Time 0.460 (0.474)	Loss 0.72 (0.64)	mAP 54.97 (66.99)
Train: [5/50]	Time 0.470	Loss 0.64 	mAP 67.05
Test: [0/50]	Time 1.824 (1.824)	Precision 65.73 (65.73)	Recall 84.78 (84.78) 	 P_C 66.11 	 R_C 80.21 	 F_C 68.71 	 P_O 65.73 	 R_O 84.78 	 F_O 74.05
Test: [5/50]	  P_C 65.86 	 R_C 81.86 	 F_C 71.87 	 P_O 68.77 	 R_O 83.96 	 F_O 75.61 	 mAP 80.72
Train: [0/157]	Time 1.065 (1.065)	Loss 0.41 (0.41)	mAP 75.69 (75.69)
Train: [100/157]	Time 0.470 (0.473)	Loss 0.62 (0.64)	mAP 69.20 (67.90)
Train: [6/50]	Time 0.471	Loss 0.63 	mAP 68.05
Test: [0/50]	Time 1.674 (1.674)	Precision 70.06 (70.06)	Recall 84.78 (84.78) 	 P_C 74.39 	 R_C 81.28 	 F_C 72.73 	 P_O 70.06 	 R_O 84.78 	 F_O 76.72
Test: [6/50]	  P_C 68.64 	 R_C 79.72 	 F_C 72.43 	 P_O 69.73 	 R_O 83.79 	 F_O 76.11 	 mAP 81.25
Train: [0/157]	Time 1.047 (1.047)	Loss 0.69 (0.69)	mAP 69.69 (69.69)
Train: [100/157]	Time 0.455 (0.471)	Loss 0.92 (0.61)	mAP 50.74 (68.96)
Train: [7/50]	Time 0.469	Loss 0.61 	mAP 68.73
Test: [0/50]	Time 1.572 (1.572)	Precision 72.96 (72.96)	Recall 84.06 (84.06) 	 P_C 69.28 	 R_C 82.92 	 F_C 73.80 	 P_O 72.96 	 R_O 84.06 	 F_O 78.11
Test: [7/50]	  P_C 67.61 	 R_C 82.07 	 F_C 73.70 	 P_O 71.53 	 R_O 83.92 	 F_O 77.23 	 mAP 82.47
Train: [0/157]	Time 1.058 (1.058)	Loss 0.68 (0.68)	mAP 58.62 (58.62)
Train: [100/157]	Time 0.492 (0.473)	Loss 0.64 (0.60)	mAP 75.69 (69.16)
Train: [8/50]	Time 0.472	Loss 0.60 	mAP 69.24
Test: [0/50]	Time 1.550 (1.550)	Precision 70.83 (70.83)	Recall 86.23 (86.23) 	 P_C 72.30 	 R_C 82.63 	 F_C 74.72 	 P_O 70.83 	 R_O 86.23 	 F_O 77.78
Test: [8/50]	  P_C 67.74 	 R_C 83.27 	 F_C 73.87 	 P_O 70.83 	 R_O 84.47 	 F_O 77.05 	 mAP 83.30
Train: [0/157]	Time 1.112 (1.112)	Loss 0.53 (0.53)	mAP 73.38 (73.38)
Train: [100/157]	Time 0.521 (0.476)	Loss 0.61 (0.58)	mAP 65.08 (69.47)
Train: [9/50]	Time 0.471	Loss 0.58 	mAP 69.18
Test: [0/50]	Time 1.559 (1.559)	Precision 70.86 (70.86)	Recall 89.86 (89.86) 	 P_C 69.73 	 R_C 87.60 	 F_C 76.25 	 P_O 70.86 	 R_O 89.86 	 F_O 79.23
Test: [9/50]	  P_C 69.12 	 R_C 82.65 	 F_C 74.82 	 P_O 72.09 	 R_O 85.30 	 F_O 78.14 	 mAP 83.32
Train: [0/157]	Time 1.081 (1.081)	Loss 0.54 (0.54)	mAP 76.82 (76.82)
Train: [100/157]	Time 0.459 (0.474)	Loss 0.50 (0.56)	mAP 79.85 (70.27)
Train: [10/50]	Time 0.471	Loss 0.56 	mAP 70.25
Test: [0/50]	Time 1.568 (1.568)	Precision 74.23 (74.23)	Recall 87.68 (87.68) 	 P_C 76.65 	 R_C 85.55 	 F_C 79.27 	 P_O 74.23 	 R_O 87.68 	 F_O 80.40
Test: [10/50]	  P_C 71.00 	 R_C 82.69 	 F_C 75.41 	 P_O 74.22 	 R_O 84.37 	 F_O 78.97 	 mAP 83.85
Train: [0/157]	Time 1.109 (1.109)	Loss 0.71 (0.71)	mAP 64.85 (64.85)
Train: [100/157]	Time 0.465 (0.474)	Loss 0.52 (0.57)	mAP 67.62 (69.58)
Train: [11/50]	Time 0.470	Loss 0.56 	mAP 69.64
Test: [0/50]	Time 1.656 (1.656)	Precision 68.75 (68.75)	Recall 87.68 (87.68) 	 P_C 76.59 	 R_C 81.11 	 F_C 75.56 	 P_O 68.75 	 R_O 87.68 	 F_O 77.07
Test: [11/50]	  P_C 70.58 	 R_C 83.78 	 F_C 75.77 	 P_O 70.62 	 R_O 87.11 	 F_O 78.01 	 mAP 84.42
Train: [0/157]	Time 1.072 (1.072)	Loss 0.54 (0.54)	mAP 64.50 (64.50)
Train: [100/157]	Time 0.511 (0.477)	Loss 0.53 (0.55)	mAP 68.79 (70.20)
Train: [12/50]	Time 0.473	Loss 0.56 	mAP 70.47
Test: [0/50]	Time 1.566 (1.566)	Precision 69.82 (69.82)	Recall 85.51 (85.51) 	 P_C 73.52 	 R_C 81.61 	 F_C 74.54 	 P_O 69.82 	 R_O 85.51 	 F_O 76.87
Test: [12/50]	  P_C 72.10 	 R_C 82.67 	 F_C 76.16 	 P_O 72.43 	 R_O 86.07 	 F_O 78.66 	 mAP 84.53
Train: [0/157]	Time 1.087 (1.087)	Loss 0.51 (0.51)	mAP 76.56 (76.56)
Train: [100/157]	Time 0.457 (0.468)	Loss 0.45 (0.54)	mAP 73.79 (71.45)
Train: [13/50]	Time 0.465	Loss 0.55 	mAP 70.66
Test: [0/50]	Time 1.584 (1.584)	Precision 67.60 (67.60)	Recall 87.68 (87.68) 	 P_C 72.71 	 R_C 85.21 	 F_C 76.41 	 P_O 67.60 	 R_O 87.68 	 F_O 76.34
Test: [13/50]	  P_C 70.80 	 R_C 84.44 	 F_C 75.94 	 P_O 70.48 	 R_O 87.21 	 F_O 77.96 	 mAP 84.56
Train: [0/157]	Time 1.032 (1.032)	Loss 0.61 (0.61)	mAP 70.04 (70.04)
Train: [100/157]	Time 0.453 (0.466)	Loss 0.51 (0.53)	mAP 51.90 (69.62)
Train: [14/50]	Time 0.464	Loss 0.54 	mAP 69.82
Test: [0/50]	Time 1.549 (1.549)	Precision 73.78 (73.78)	Recall 87.68 (87.68) 	 P_C 74.68 	 R_C 85.77 	 F_C 77.71 	 P_O 73.78 	 R_O 87.68 	 F_O 80.13
Test: [14/50]	  P_C 71.02 	 R_C 84.31 	 F_C 76.60 	 P_O 74.39 	 R_O 85.73 	 F_O 79.66 	 mAP 85.00
Train: [0/157]	Time 1.027 (1.027)	Loss 0.56 (0.56)	mAP 80.17 (80.17)
Train: [100/157]	Time 0.454 (0.470)	Loss 0.72 (0.52)	mAP 72.05 (71.90)
Train: [15/50]	Time 0.466	Loss 0.54 	mAP 71.47
Test: [0/50]	Time 1.543 (1.543)	Precision 63.49 (63.49)	Recall 86.96 (86.96) 	 P_C 71.96 	 R_C 83.71 	 F_C 74.09 	 P_O 63.49 	 R_O 86.96 	 F_O 73.39
Test: [15/50]	  P_C 71.16 	 R_C 84.35 	 F_C 76.38 	 P_O 73.97 	 R_O 86.18 	 F_O 79.61 	 mAP 85.15
Train: [0/157]	Time 1.048 (1.048)	Loss 0.60 (0.60)	mAP 52.44 (52.44)
Train: [100/157]	Time 0.457 (0.467)	Loss 0.64 (0.54)	mAP 81.10 (70.77)
Train: [16/50]	Time 0.464	Loss 0.54 	mAP 70.89
Test: [0/50]	Time 1.616 (1.616)	Precision 76.77 (76.77)	Recall 86.23 (86.23) 	 P_C 79.21 	 R_C 85.24 	 F_C 80.71 	 P_O 76.77 	 R_O 86.23 	 F_O 81.23
Test: [16/50]	  P_C 73.32 	 R_C 84.22 	 F_C 77.86 	 P_O 75.72 	 R_O 85.74 	 F_O 80.42 	 mAP 85.32
Train: [0/157]	Time 1.052 (1.052)	Loss 0.37 (0.37)	mAP 76.29 (76.29)
Train: [100/157]	Time 0.460 (0.468)	Loss 0.45 (0.52)	mAP 66.77 (70.80)
Train: [17/50]	Time 0.466	Loss 0.53 	mAP 70.70
Test: [0/50]	Time 1.515 (1.515)	Precision 80.13 (80.13)	Recall 87.68 (87.68) 	 P_C 87.02 	 R_C 85.10 	 F_C 83.88 	 P_O 80.13 	 R_O 87.68 	 F_O 83.74
Test: [17/50]	  P_C 74.23 	 R_C 82.03 	 F_C 77.36 	 P_O 75.61 	 R_O 85.27 	 F_O 80.15 	 mAP 85.41
Train: [0/157]	Time 1.079 (1.079)	Loss 0.43 (0.43)	mAP 76.42 (76.42)
Train: [100/157]	Time 0.461 (0.469)	Loss 0.43 (0.52)	mAP 64.63 (70.26)
Train: [18/50]	Time 0.465	Loss 0.52 	mAP 70.71
Test: [0/50]	Time 1.550 (1.550)	Precision 72.46 (72.46)	Recall 87.68 (87.68) 	 P_C 74.31 	 R_C 86.05 	 F_C 77.99 	 P_O 72.46 	 R_O 87.68 	 F_O 79.34
Test: [18/50]	  P_C 70.74 	 R_C 85.35 	 F_C 76.69 	 P_O 72.09 	 R_O 88.09 	 F_O 79.29 	 mAP 85.52
Train: [0/157]	Time 1.048 (1.048)	Loss 0.50 (0.50)	mAP 75.24 (75.24)
Train: [100/157]	Time 0.452 (0.466)	Loss 0.70 (0.53)	mAP 64.08 (70.68)
Train: [19/50]	Time 0.463	Loss 0.53 	mAP 70.67
Test: [0/50]	Time 1.629 (1.629)	Precision 70.45 (70.45)	Recall 89.86 (89.86) 	 P_C 74.52 	 R_C 89.71 	 F_C 78.75 	 P_O 70.45 	 R_O 89.86 	 F_O 78.98
Test: [19/50]	  P_C 68.95 	 R_C 86.25 	 F_C 75.75 	 P_O 70.23 	 R_O 88.31 	 F_O 78.24 	 mAP 85.55
Train: [0/157]	Time 1.006 (1.006)	Loss 0.40 (0.40)	mAP 65.95 (65.95)
Train: [100/157]	Time 0.458 (0.468)	Loss 0.48 (0.51)	mAP 66.64 (70.09)
Train: [20/50]	Time 0.466	Loss 0.51 	mAP 70.72
Test: [0/50]	Time 1.558 (1.558)	Precision 72.02 (72.02)	Recall 87.68 (87.68) 	 P_C 75.15 	 R_C 85.30 	 F_C 77.22 	 P_O 72.02 	 R_O 87.68 	 F_O 79.08
Test: [20/50]	  P_C 69.15 	 R_C 86.86 	 F_C 76.08 	 P_O 70.66 	 R_O 88.38 	 F_O 78.53 	 mAP 85.70
Train: [0/157]	Time 1.079 (1.079)	Loss 0.50 (0.50)	mAP 78.21 (78.21)
Train: [100/157]	Time 0.456 (0.468)	Loss 0.58 (0.51)	mAP 68.46 (71.66)
Train: [21/50]	Time 0.464	Loss 0.51 	mAP 71.21
Test: [0/50]	Time 1.553 (1.553)	Precision 65.08 (65.08)	Recall 89.13 (89.13) 	 P_C 67.19 	 R_C 86.61 	 F_C 73.39 	 P_O 65.08 	 R_O 89.13 	 F_O 75.23
Test: [21/50]	  P_C 69.77 	 R_C 86.13 	 F_C 76.41 	 P_O 70.63 	 R_O 88.74 	 F_O 78.65 	 mAP 86.06
Train: [0/157]	Time 1.000 (1.000)	Loss 0.52 (0.52)	mAP 76.67 (76.67)
Train: [100/157]	Time 0.457 (0.469)	Loss 0.59 (0.51)	mAP 60.34 (70.69)
Train: [22/50]	Time 0.467	Loss 0.51 	mAP 71.52
Test: [0/50]	Time 1.669 (1.669)	Precision 74.69 (74.69)	Recall 87.68 (87.68) 	 P_C 76.79 	 R_C 86.73 	 F_C 80.10 	 P_O 74.69 	 R_O 87.68 	 F_O 80.67
Test: [22/50]	  P_C 70.95 	 R_C 85.98 	 F_C 76.70 	 P_O 71.22 	 R_O 88.34 	 F_O 78.86 	 mAP 86.01
Train: [0/157]	Time 1.051 (1.051)	Loss 0.48 (0.48)	mAP 83.64 (83.64)
Train: [100/157]	Time 0.457 (0.467)	Loss 0.47 (0.50)	mAP 69.27 (71.91)
Train: [23/50]	Time 0.464	Loss 0.51 	mAP 71.74
Test: [0/50]	Time 1.623 (1.623)	Precision 78.71 (78.71)	Recall 88.41 (88.41) 	 P_C 79.64 	 R_C 87.64 	 F_C 81.77 	 P_O 78.71 	 R_O 88.41 	 F_O 83.28
Test: [23/50]	  P_C 74.03 	 R_C 83.59 	 F_C 78.06 	 P_O 76.92 	 R_O 84.83 	 F_O 80.68 	 mAP 85.86
Train: [0/157]	Time 1.050 (1.050)	Loss 0.43 (0.43)	mAP 75.74 (75.74)
Train: [100/157]	Time 0.454 (0.468)	Loss 0.61 (0.50)	mAP 74.86 (71.28)
Train: [24/50]	Time 0.465	Loss 0.50 	mAP 71.55
Test: [0/50]	Time 1.499 (1.499)	Precision 70.06 (70.06)	Recall 89.86 (89.86) 	 P_C 74.10 	 R_C 88.70 	 F_C 79.16 	 P_O 70.06 	 R_O 89.86 	 F_O 78.73
Test: [24/50]	  P_C 71.86 	 R_C 84.96 	 F_C 77.05 	 P_O 72.52 	 R_O 87.69 	 F_O 79.39 	 mAP 85.91
Train: [0/157]	Time 1.019 (1.019)	Loss 0.59 (0.59)	mAP 63.99 (63.99)
Train: [100/157]	Time 0.459 (0.470)	Loss 0.39 (0.49)	mAP 57.28 (72.31)
Train: [25/50]	Time 0.467	Loss 0.49 	mAP 71.83
Test: [0/50]	Time 1.548 (1.548)	Precision 76.25 (76.25)	Recall 88.41 (88.41) 	 P_C 79.06 	 R_C 85.53 	 F_C 79.83 	 P_O 76.25 	 R_O 88.41 	 F_O 81.88
Test: [25/50]	  P_C 74.96 	 R_C 84.73 	 F_C 78.95 	 P_O 77.19 	 R_O 86.44 	 F_O 81.56 	 mAP 86.67
Train: [0/157]	Time 1.006 (1.006)	Loss 0.52 (0.52)	mAP 75.67 (75.67)
Train: [100/157]	Time 0.450 (0.465)	Loss 0.61 (0.50)	mAP 81.61 (72.17)
Train: [26/50]	Time 0.464	Loss 0.50 	mAP 71.87
Test: [0/50]	Time 1.592 (1.592)	Precision 74.68 (74.68)	Recall 85.51 (85.51) 	 P_C 79.37 	 R_C 81.88 	 F_C 77.84 	 P_O 74.68 	 R_O 85.51 	 F_O 79.73
Test: [26/50]	  P_C 71.78 	 R_C 85.88 	 F_C 77.74 	 P_O 74.56 	 R_O 87.79 	 F_O 80.64 	 mAP 86.37
Train: [0/157]	Time 1.028 (1.028)	Loss 0.57 (0.57)	mAP 69.44 (69.44)
Train: [100/157]	Time 0.456 (0.467)	Loss 0.50 (0.51)	mAP 76.86 (71.71)
Train: [27/50]	Time 0.466	Loss 0.49 	mAP 70.81
Test: [0/50]	Time 1.554 (1.554)	Precision 68.97 (68.97)	Recall 86.96 (86.96) 	 P_C 79.08 	 R_C 82.00 	 F_C 77.66 	 P_O 68.97 	 R_O 86.96 	 F_O 76.92
Test: [27/50]	  P_C 74.04 	 R_C 84.56 	 F_C 78.63 	 P_O 74.69 	 R_O 87.38 	 F_O 80.54 	 mAP 86.42
Train: [0/157]	Time 1.075 (1.075)	Loss 0.48 (0.48)	mAP 68.96 (68.96)
Train: [100/157]	Time 0.457 (0.469)	Loss 0.45 (0.50)	mAP 81.30 (72.85)
Train: [28/50]	Time 0.466	Loss 0.50 	mAP 72.87
Test: [0/50]	Time 1.542 (1.542)	Precision 73.78 (73.78)	Recall 87.68 (87.68) 	 P_C 76.04 	 R_C 83.70 	 F_C 78.16 	 P_O 73.78 	 R_O 87.68 	 F_O 80.13
Test: [28/50]	  P_C 72.06 	 R_C 86.09 	 F_C 77.81 	 P_O 72.94 	 R_O 87.84 	 F_O 79.70 	 mAP 86.65
Train: [0/157]	Time 1.012 (1.012)	Loss 0.43 (0.43)	mAP 77.41 (77.41)
Train: [100/157]	Time 0.524 (0.469)	Loss 0.63 (0.48)	mAP 80.10 (72.71)
Train: [29/50]	Time 0.465	Loss 0.48 	mAP 71.94
Test: [0/50]	Time 1.584 (1.584)	Precision 75.00 (75.00)	Recall 89.13 (89.13) 	 P_C 81.39 	 R_C 85.42 	 F_C 81.44 	 P_O 75.00 	 R_O 89.13 	 F_O 81.46
Test: [29/50]	  P_C 74.71 	 R_C 83.91 	 F_C 78.62 	 P_O 75.77 	 R_O 86.60 	 F_O 80.82 	 mAP 86.75
Train: [0/157]	Time 1.063 (1.063)	Loss 0.65 (0.65)	mAP 57.69 (57.69)
Train: [100/157]	Time 0.460 (0.471)	Loss 0.46 (0.48)	mAP 66.74 (72.50)
Train: [30/50]	Time 0.469	Loss 0.48 	mAP 71.81
Test: [0/50]	Time 1.536 (1.536)	Precision 72.73 (72.73)	Recall 86.96 (86.96) 	 P_C 73.19 	 R_C 86.74 	 F_C 78.39 	 P_O 72.73 	 R_O 86.96 	 F_O 79.21
Test: [30/50]	  P_C 72.11 	 R_C 86.35 	 F_C 78.05 	 P_O 74.91 	 R_O 87.85 	 F_O 80.87 	 mAP 86.78
Train: [0/157]	Time 1.069 (1.069)	Loss 0.50 (0.50)	mAP 75.18 (75.18)
Train: [100/157]	Time 0.459 (0.468)	Loss 0.54 (0.49)	mAP 82.16 (72.52)
Train: [31/50]	Time 0.465	Loss 0.48 	mAP 71.96
Test: [0/50]	Time 1.561 (1.561)	Precision 68.57 (68.57)	Recall 86.96 (86.96) 	 P_C 74.96 	 R_C 84.05 	 F_C 77.18 	 P_O 68.57 	 R_O 86.96 	 F_O 76.68
Test: [31/50]	  P_C 73.28 	 R_C 85.12 	 F_C 78.25 	 P_O 75.90 	 R_O 86.84 	 F_O 81.00 	 mAP 86.64
Train: [0/157]	Time 1.047 (1.047)	Loss 0.34 (0.34)	mAP 82.38 (82.38)
Train: [100/157]	Time 0.462 (0.468)	Loss 0.40 (0.48)	mAP 69.29 (71.59)
Train: [32/50]	Time 0.465	Loss 0.48 	mAP 71.49
Test: [0/50]	Time 1.627 (1.627)	Precision 77.22 (77.22)	Recall 88.41 (88.41) 	 P_C 79.86 	 R_C 88.38 	 F_C 81.65 	 P_O 77.22 	 R_O 88.41 	 F_O 82.43
Test: [32/50]	  P_C 74.42 	 R_C 84.94 	 F_C 78.67 	 P_O 77.41 	 R_O 85.95 	 F_O 81.46 	 mAP 86.96
Train: [0/157]	Time 1.064 (1.064)	Loss 0.48 (0.48)	mAP 72.93 (72.93)
Train: [100/157]	Time 0.455 (0.471)	Loss 0.53 (0.48)	mAP 54.96 (71.75)
Train: [33/50]	Time 0.467	Loss 0.48 	mAP 71.61
Test: [0/50]	Time 1.614 (1.614)	Precision 73.94 (73.94)	Recall 88.41 (88.41) 	 P_C 77.91 	 R_C 86.98 	 F_C 80.74 	 P_O 73.94 	 R_O 88.41 	 F_O 80.53
Test: [33/50]	  P_C 72.78 	 R_C 86.45 	 F_C 78.16 	 P_O 73.37 	 R_O 88.19 	 F_O 80.10 	 mAP 86.76
Train: [0/157]	Time 0.983 (0.983)	Loss 0.64 (0.64)	mAP 72.76 (72.76)
Train: [100/157]	Time 0.448 (0.463)	Loss 0.38 (0.48)	mAP 67.81 (72.29)
Train: [34/50]	Time 0.459	Loss 0.48 	mAP 71.99
Test: [0/50]	Time 1.488 (1.488)	Precision 78.48 (78.48)	Recall 89.86 (89.86) 	 P_C 81.94 	 R_C 87.86 	 F_C 83.01 	 P_O 78.48 	 R_O 89.86 	 F_O 83.78
Test: [34/50]	  P_C 73.88 	 R_C 85.23 	 F_C 78.65 	 P_O 75.29 	 R_O 87.52 	 F_O 80.95 	 mAP 86.77
Train: [0/157]	Time 1.018 (1.018)	Loss 0.49 (0.49)	mAP 68.30 (68.30)
Train: [100/157]	Time 0.454 (0.460)	Loss 0.53 (0.48)	mAP 79.89 (72.42)
Train: [35/50]	Time 0.458	Loss 0.48 	mAP 72.73
Test: [0/50]	Time 1.479 (1.479)	Precision 72.09 (72.09)	Recall 89.86 (89.86) 	 P_C 80.23 	 R_C 86.27 	 F_C 81.12 	 P_O 72.09 	 R_O 89.86 	 F_O 80.00
Test: [35/50]	  P_C 73.03 	 R_C 85.93 	 F_C 78.57 	 P_O 75.16 	 R_O 87.89 	 F_O 81.03 	 mAP 86.68
Train: [0/157]	Time 0.987 (0.987)	Loss 0.51 (0.51)	mAP 80.77 (80.77)
Train: [100/157]	Time 0.448 (0.463)	Loss 0.44 (0.47)	mAP 82.20 (72.21)
Train: [36/50]	Time 0.460	Loss 0.47 	mAP 71.81
Test: [0/50]	Time 1.548 (1.548)	Precision 70.69 (70.69)	Recall 89.13 (89.13) 	 P_C 73.10 	 R_C 85.58 	 F_C 77.05 	 P_O 70.69 	 R_O 89.13 	 F_O 78.85
Test: [36/50]	  P_C 72.19 	 R_C 85.87 	 F_C 77.99 	 P_O 73.65 	 R_O 88.52 	 F_O 80.40 	 mAP 86.78
Train: [0/157]	Time 1.054 (1.054)	Loss 0.51 (0.51)	mAP 68.82 (68.82)
Train: [100/157]	Time 0.462 (0.464)	Loss 0.45 (0.48)	mAP 83.06 (69.56)
Train: [37/50]	Time 0.459	Loss 0.47 	mAP 71.06
Test: [0/50]	Time 1.488 (1.488)	Precision 72.56 (72.56)	Recall 86.23 (86.23) 	 P_C 78.08 	 R_C 86.56 	 F_C 79.51 	 P_O 72.56 	 R_O 86.23 	 F_O 78.81
Test: [37/50]	  P_C 74.61 	 R_C 85.36 	 F_C 79.19 	 P_O 76.25 	 R_O 87.55 	 F_O 81.51 	 mAP 87.09
Train: [0/157]	Time 1.010 (1.010)	Loss 0.41 (0.41)	mAP 56.46 (56.46)
Train: [100/157]	Time 0.451 (0.459)	Loss 0.43 (0.48)	mAP 61.27 (72.06)
Train: [38/50]	Time 0.457	Loss 0.48 	mAP 72.04
Test: [0/50]	Time 1.489 (1.489)	Precision 74.07 (74.07)	Recall 86.96 (86.96) 	 P_C 76.96 	 R_C 84.30 	 F_C 78.19 	 P_O 74.07 	 R_O 86.96 	 F_O 80.00
Test: [38/50]	  P_C 74.29 	 R_C 86.03 	 F_C 79.33 	 P_O 75.42 	 R_O 88.35 	 F_O 81.38 	 mAP 87.10
Train: [0/157]	Time 1.026 (1.026)	Loss 0.55 (0.55)	mAP 78.76 (78.76)
Train: [100/157]	Time 0.453 (0.464)	Loss 0.60 (0.47)	mAP 70.74 (71.94)
Train: [39/50]	Time 0.460	Loss 0.47 	mAP 71.99
Test: [0/50]	Time 1.540 (1.540)	Precision 63.86 (63.86)	Recall 93.48 (93.48) 	 P_C 70.41 	 R_C 93.02 	 F_C 78.26 	 P_O 63.86 	 R_O 93.48 	 F_O 75.88
Test: [39/50]	  P_C 73.55 	 R_C 86.15 	 F_C 78.86 	 P_O 75.15 	 R_O 88.02 	 F_O 81.08 	 mAP 87.00
Train: [0/157]	Time 0.985 (0.985)	Loss 0.45 (0.45)	mAP 75.63 (75.63)
Train: [100/157]	Time 0.450 (0.457)	Loss 0.43 (0.48)	mAP 68.52 (73.04)
Train: [40/50]	Time 0.455	Loss 0.47 	mAP 72.55
Test: [0/50]	Time 1.480 (1.480)	Precision 76.73 (76.73)	Recall 88.41 (88.41) 	 P_C 81.94 	 R_C 85.11 	 F_C 80.97 	 P_O 76.73 	 R_O 88.41 	 F_O 82.15
Test: [40/50]	  P_C 74.12 	 R_C 85.13 	 F_C 78.94 	 P_O 76.96 	 R_O 87.27 	 F_O 81.79 	 mAP 86.80
Train: [0/157]	Time 1.010 (1.010)	Loss 0.43 (0.43)	mAP 72.71 (72.71)
Train: [100/157]	Time 0.451 (0.459)	Loss 0.39 (0.45)	mAP 79.44 (71.23)
Train: [41/50]	Time 0.457	Loss 0.46 	mAP 71.15
Test: [0/50]	Time 1.497 (1.497)	Precision 70.69 (70.69)	Recall 89.13 (89.13) 	 P_C 76.08 	 R_C 87.52 	 F_C 79.21 	 P_O 70.69 	 R_O 89.13 	 F_O 78.85
Test: [41/50]	  P_C 73.24 	 R_C 85.98 	 F_C 78.69 	 P_O 76.09 	 R_O 87.67 	 F_O 81.47 	 mAP 87.00
Train: [0/157]	Time 0.999 (0.999)	Loss 0.64 (0.64)	mAP 78.30 (78.30)
Train: [100/157]	Time 0.451 (0.462)	Loss 0.43 (0.46)	mAP 65.72 (72.26)
Train: [42/50]	Time 0.459	Loss 0.46 	mAP 71.54
Test: [0/50]	Time 1.486 (1.486)	Precision 74.23 (74.23)	Recall 87.68 (87.68) 	 P_C 79.33 	 R_C 84.45 	 F_C 79.84 	 P_O 74.23 	 R_O 87.68 	 F_O 80.40
Test: [42/50]	  P_C 75.17 	 R_C 85.40 	 F_C 79.62 	 P_O 76.97 	 R_O 87.71 	 F_O 81.99 	 mAP 87.08
Train: [0/157]	Time 1.018 (1.018)	Loss 0.49 (0.49)	mAP 77.99 (77.99)
Train: [100/157]	Time 0.451 (0.458)	Loss 0.45 (0.46)	mAP 80.83 (71.49)
Train: [43/50]	Time 0.456	Loss 0.46 	mAP 71.75
Test: [0/50]	Time 1.499 (1.499)	Precision 71.43 (71.43)	Recall 90.58 (90.58) 	 P_C 77.71 	 R_C 88.45 	 F_C 80.74 	 P_O 71.43 	 R_O 90.58 	 F_O 79.87
Test: [43/50]	  P_C 74.28 	 R_C 86.11 	 F_C 79.41 	 P_O 76.32 	 R_O 87.64 	 F_O 81.59 	 mAP 87.28
Train: [0/157]	Time 0.991 (0.991)	Loss 0.35 (0.35)	mAP 79.05 (79.05)
Train: [100/157]	Time 0.458 (0.459)	Loss 0.47 (0.46)	mAP 67.14 (72.66)
Train: [44/50]	Time 0.458	Loss 0.45 	mAP 72.60
Test: [0/50]	Time 1.478 (1.478)	Precision 80.67 (80.67)	Recall 87.68 (87.68) 	 P_C 84.71 	 R_C 86.64 	 F_C 83.66 	 P_O 80.67 	 R_O 87.68 	 F_O 84.03
Test: [44/50]	  P_C 75.15 	 R_C 86.11 	 F_C 79.90 	 P_O 76.57 	 R_O 87.79 	 F_O 81.80 	 mAP 87.30
Train: [0/157]	Time 1.028 (1.028)	Loss 0.32 (0.32)	mAP 85.42 (85.42)
Train: [100/157]	Time 0.458 (0.463)	Loss 0.46 (0.46)	mAP 68.32 (71.92)
Train: [45/50]	Time 0.459	Loss 0.46 	mAP 72.16
Test: [0/50]	Time 1.480 (1.480)	Precision 79.19 (79.19)	Recall 85.51 (85.51) 	 P_C 84.23 	 R_C 81.14 	 F_C 79.58 	 P_O 79.19 	 R_O 85.51 	 F_O 82.23
Test: [45/50]	  P_C 74.66 	 R_C 85.79 	 F_C 79.56 	 P_O 77.70 	 R_O 87.31 	 F_O 82.23 	 mAP 87.02
Train: [0/157]	Time 1.055 (1.055)	Loss 0.47 (0.47)	mAP 73.34 (73.34)
Train: [100/157]	Time 0.459 (0.461)	Loss 0.52 (0.46)	mAP 76.97 (72.37)
Train: [46/50]	Time 0.458	Loss 0.46 	mAP 72.97
Test: [0/50]	Time 1.530 (1.530)	Precision 73.96 (73.96)	Recall 90.58 (90.58) 	 P_C 78.89 	 R_C 89.46 	 F_C 81.26 	 P_O 73.96 	 R_O 90.58 	 F_O 81.43
Test: [46/50]	  P_C 74.02 	 R_C 85.97 	 F_C 79.17 	 P_O 76.09 	 R_O 88.21 	 F_O 81.70 	 mAP 87.10
Train: [0/157]	Time 0.983 (0.983)	Loss 0.60 (0.60)	mAP 70.93 (70.93)
Train: [100/157]	Time 0.451 (0.461)	Loss 0.54 (0.46)	mAP 74.64 (72.04)
Train: [47/50]	Time 0.458	Loss 0.46 	mAP 72.36
Test: [0/50]	Time 1.488 (1.488)	Precision 76.77 (76.77)	Recall 86.23 (86.23) 	 P_C 81.25 	 R_C 84.14 	 F_C 80.83 	 P_O 76.77 	 R_O 86.23 	 F_O 81.23
Test: [47/50]	  P_C 75.56 	 R_C 85.61 	 F_C 79.90 	 P_O 77.44 	 R_O 87.44 	 F_O 82.14 	 mAP 87.11
Train: [0/157]	Time 1.056 (1.056)	Loss 0.39 (0.39)	mAP 69.48 (69.48)
Train: [100/157]	Time 0.461 (0.465)	Loss 0.61 (0.46)	mAP 63.24 (71.50)
Train: [48/50]	Time 0.461	Loss 0.46 	mAP 71.50
Test: [0/50]	Time 1.481 (1.481)	Precision 77.85 (77.85)	Recall 89.13 (89.13) 	 P_C 85.03 	 R_C 85.43 	 F_C 83.26 	 P_O 77.85 	 R_O 89.13 	 F_O 83.11
Test: [48/50]	  P_C 74.26 	 R_C 85.45 	 F_C 79.16 	 P_O 76.06 	 R_O 87.67 	 F_O 81.45 	 mAP 86.93
Train: [0/157]	Time 1.002 (1.002)	Loss 0.48 (0.48)	mAP 86.31 (86.31)
Train: [100/157]	Time 0.458 (0.460)	Loss 0.43 (0.46)	mAP 84.47 (72.09)
Train: [49/50]	Time 0.457	Loss 0.46 	mAP 72.63
Test: [0/50]	Time 1.535 (1.535)	Precision 78.29 (78.29)	Recall 86.23 (86.23) 	 P_C 82.52 	 R_C 85.71 	 F_C 81.25 	 P_O 78.29 	 R_O 86.23 	 F_O 82.07
Test: [49/50]	  P_C 75.19 	 R_C 85.61 	 F_C 79.76 	 P_O 76.97 	 R_O 87.81 	 F_O 82.04 	 mAP 87.31
Train: [0/157]	Time 1.036 (1.036)	Loss 0.53 (0.53)	mAP 65.13 (65.13)
Train: [100/157]	Time 0.455 (0.459)	Loss 0.53 (0.46)	mAP 72.19 (73.27)
Train: [50/50]	Time 0.457	Loss 0.45 	mAP 72.81
Test: [0/50]	Time 1.482 (1.482)	Precision 78.57 (78.57)	Recall 87.68 (87.68) 	 P_C 83.15 	 R_C 85.98 	 F_C 82.42 	 P_O 78.57 	 R_O 87.68 	 F_O 82.88
Test: [50/50]	  P_C 74.93 	 R_C 85.73 	 F_C 79.71 	 P_O 77.38 	 R_O 87.51 	 F_O 82.13 	 mAP 87.60
Evaluating the best model
Evaluate with threshold 0.50
... loading pretrained weights from ./output/voc2007-RN101-cosine-bs32-e50/model_best.pth.tar
Test: [0/50]	Time 1.511 (1.511)	Precision 76.25 (76.25)	Recall 88.41 (88.41) 	 P_C 80.73 	 R_C 84.93 	 F_C 81.17 	 P_O 76.25 	 R_O 88.41 	 F_O 81.88
Test: [50/50]	  P_C 74.25 	 R_C 85.39 	 F_C 79.11 	 P_O 76.50 	 R_O 87.15 	 F_O 81.48 	 mAP 87.14

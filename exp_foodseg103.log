nohup: ignoring input
 -------------------- Building Dataset ----------------------
DATASET.ROOT = /home/samyakr2/food/FoodSeg103/Images
data_split = trainval
PARTIAL_PORTION= 1.000000
INPUT.SIZE = 224
 -------------------- Building Dataset ----------------------
DATASET.ROOT = /home/samyakr2/food/FoodSeg103/Images
data_split = test
PARTIAL_PORTION= 1.000000
INPUT.SIZE = 224
 -------------------- Building Dataset ----------------------
DATASET.ROOT = /home/samyakr2/food/FoodSeg103/Images
data_split = test
PARTIAL_PORTION= 1.000000
INPUT.SIZE = 224
Loading CLIP (backbone: RN101)
Building dualcoop
Initializing class-specific contexts
Initial positive context: "X X X X X X X X X X X X X X X X"
Initial negative  context: "X X X X X X X X X X X X X X X X"
Number of positive context words (tokens): 16
Number of negative context words (tokens): 16
Freeze the backbone weights
Freeze the attn weights
image_encoder.attnpool.positional_embedding
image_encoder.attnpool.k_proj.weight
image_encoder.attnpool.k_proj.bias
image_encoder.attnpool.q_proj.weight
image_encoder.attnpool.q_proj.bias
image_encoder.attnpool.v_proj.weight
image_encoder.attnpool.v_proj.bias
image_encoder.attnpool.c_proj.weight
image_encoder.attnpool.c_proj.bias
Multiple GPUs detected (n_gpus=2), use all of them!
num of params in prompt learner:  2
train.py --config_file configs/models/rn101_ep50.yaml --datadir /home/samyakr2/food/FoodSeg103/Images --dataset_config_file configs/datasets/foodseg103.yaml --input_size 224 --lr 0.001 --loss_w 0.03 -pp 1 --csc
Namespace(prefix='', resume=None, pretrained=None, auto_resume=False, datadir='/home/samyakr2/food/FoodSeg103/Images', input_size=224, train_input_size=None, num_train_cls=100, test_input_size=None, thre=0.5, single_prompt='pos', output_dir='', print_freq=100, val_freq_in_epoch=-1, evaluate=False, config_file='configs/models/rn101_ep50.yaml', dataset_config_file='configs/datasets/foodseg103.yaml', positive_prompt=None, negative_prompt=None, n_ctx_pos=None, n_ctx_neg=None, lr=0.001, loss_w=0.03, csc=True, logit_scale=100.0, gamma_neg=2.0, gamma_pos=1.0, portion=1.0, partial_portion=1.0, mask_file=None, train_batch_size=None, stop_epochs=None, max_epochs=None, finetune=False, finetune_backbone=False, finetune_attn=False, finetune_text=False, base_lr_mult=None, backbone_lr_mult=None, text_lr_mult=None, attn_lr_mult=None, val_every_n_epochs=1, warmup_epochs=1, top_k=3)
DataParallel(
  (module): DualCoop(
    (prompt_learner): MLCPromptLearner()
    (image_encoder): ModifiedResNet_conv_proj(
      (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
      (relu): ReLU(inplace=True)
      (layer1): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (-1): AvgPool2d(kernel_size=1, stride=1, padding=0)
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (layer2): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (layer3): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (6): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (7): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (8): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (9): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (10): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (11): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (12): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (13): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (14): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (15): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (16): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (17): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (18): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (19): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (20): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (21): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (22): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (layer4): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (attnpool): AttentionConv(
        (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
        (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
        (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
        (c_proj): Linear(in_features=2048, out_features=512, bias=True)
      )
    )
    (text_encoder): TextEncoder(
      (transformer): Transformer(
        (resblocks): Sequential(
          (0): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (1): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (2): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (3): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (4): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (5): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (6): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (7): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (8): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (9): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (10): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (11): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
  )
)
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 3
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
    SHUFFLE: False
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    PARTIAL_PORTION: 1.0
    PORTION: 1.0
    SAMPLER: RandomSampler
    SHUFFLE: True
  VAL:
    BATCH_SIZE: 100
    SHUFFLE: False
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  MASK_FILE: None
  NAME: foodseg103
  NUM_LABELED: -1
  NUM_SHOTS: -1
  ROOT: /home/samyakr2/food/FoodSeg103/Images
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  TARGET_DOMAINS: ()
  TEST_GZSL_SPLIT: 
  TEST_SPLIT: test
  TRAIN_SPLIT: trainval
  VAL_GZSL_SPLIT: 
  VAL_PERCENT: 0.1
  VAL_SPLIT: test
  ZS_TEST: 
  ZS_TEST_UNSEEN: 
  ZS_TRAIN: 
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TEST:
    SIZE: (224, 224)
  TRAIN:
    SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MLCCLIP:
  FLOAT: False
  NEGATIVE_PROMPT: 
  POSITIVE_PROMPT: 
MODEL:
  BACKBONE:
    NAME: RN101
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  ATTN_LR_MULT: 0.1
  BACKBONE_LR_MULT: 0.1
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.001
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: ./output
RESUME: 
SEED: -1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 100
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COOP_MLC:
    ASL_GAMMA_NEG: 2.0
    ASL_GAMMA_POS: 1.0
    CSC: True
    LS: 100.0
    NEGATIVE_PROMPT_INIT: 
    N_CTX_NEG: 16
    N_CTX_POS: 16
    POSITIVE_PROMPT_INIT: 
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FINETUNE: False
  FINETUNE_ATTN: False
  FINETUNE_BACKBONE: False
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: 
  RESNET_IMAGENET:
    DEPTH: 50
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
/home/samyakr2/anaconda3/lib/python3.11/site-packages/torch/_tensor.py:836: UserWarning: non-inplace resize is deprecated
  warnings.warn("non-inplace resize is deprecated")
Train: [0/156]	Time 3.058 (3.058)	Loss 70.34 (70.34)	mAP 7.07 (7.07)
Train: [100/156]	Time 0.336 (0.367)	Loss 5.71 (9.99)	mAP 10.79 (9.38)
Train: [1/50]	Time 0.358	Loss 8.37 	mAP 10.58
Test: [0/22]	Time 0.925 (0.925)	Precision 18.28 (18.28)	Recall 49.74 (49.74) 	 P_C 9.58 	 R_C 22.13 	 F_C 12.36 	 P_O 18.28 	 R_O 49.74 	 F_O 26.73
Test: [1/50]	  P_C 8.48 	 R_C 23.54 	 F_C 11.83 	 P_O 16.60 	 R_O 47.59 	 F_O 24.61 	 mAP 11.47
Train: [0/156]	Time 0.947 (0.947)	Loss 5.43 (5.43)	mAP 13.66 (13.66)
Train: [100/156]	Time 0.343 (0.349)	Loss 3.83 (4.93)	mAP 24.84 (19.94)
Train: [2/50]	Time 0.347	Loss 4.42 	mAP 20.87
Test: [0/22]	Time 0.868 (0.868)	Precision 39.80 (39.80)	Recall 63.16 (63.16) 	 P_C 23.23 	 R_C 29.86 	 F_C 24.50 	 P_O 39.80 	 R_O 63.16 	 F_O 48.83
Test: [2/50]	  P_C 26.10 	 R_C 41.90 	 F_C 28.41 	 P_O 36.02 	 R_O 63.62 	 F_O 46.00 	 mAP 34.16
Train: [0/156]	Time 0.812 (0.812)	Loss 3.20 (3.20)	mAP 25.11 (25.11)
Train: [100/156]	Time 0.343 (0.351)	Loss 3.35 (3.17)	mAP 25.76 (25.36)
Train: [3/50]	Time 0.349	Loss 3.14 	mAP 25.39
Test: [0/22]	Time 0.870 (0.870)	Precision 49.48 (49.48)	Recall 62.89 (62.89) 	 P_C 22.85 	 R_C 29.08 	 F_C 24.31 	 P_O 49.48 	 R_O 62.89 	 F_O 55.39
Test: [3/50]	  P_C 32.85 	 R_C 43.06 	 F_C 33.20 	 P_O 45.10 	 R_O 63.08 	 F_O 52.59 	 mAP 37.40
Train: [0/156]	Time 1.027 (1.027)	Loss 3.10 (3.10)	mAP 26.29 (26.29)
Train: [100/156]	Time 0.342 (0.354)	Loss 2.80 (2.95)	mAP 26.78 (26.93)
Train: [4/50]	Time 0.350	Loss 2.94 	mAP 26.98
Test: [0/22]	Time 0.877 (0.877)	Precision 50.92 (50.92)	Recall 65.26 (65.26) 	 P_C 25.33 	 R_C 29.93 	 F_C 25.81 	 P_O 50.92 	 R_O 65.26 	 F_O 57.21
Test: [4/50]	  P_C 34.03 	 R_C 46.93 	 F_C 36.43 	 P_O 46.49 	 R_O 65.45 	 F_O 54.36 	 mAP 40.02
Train: [0/156]	Time 0.820 (0.820)	Loss 2.57 (2.57)	mAP 27.77 (27.77)
Train: [100/156]	Time 0.342 (0.350)	Loss 2.85 (2.83)	mAP 26.33 (27.91)
Train: [5/50]	Time 0.348	Loss 2.82 	mAP 27.94
Test: [0/22]	Time 0.877 (0.877)	Precision 52.16 (52.16)	Recall 63.42 (63.42) 	 P_C 26.07 	 R_C 30.41 	 F_C 26.55 	 P_O 52.16 	 R_O 63.42 	 F_O 57.24
Test: [5/50]	  P_C 35.21 	 R_C 46.72 	 F_C 37.56 	 P_O 48.84 	 R_O 65.32 	 F_O 55.89 	 mAP 41.31
Train: [0/156]	Time 1.113 (1.113)	Loss 3.08 (3.08)	mAP 31.89 (31.89)
Train: [100/156]	Time 0.347 (0.354)	Loss 2.65 (2.77)	mAP 32.44 (28.37)
Train: [6/50]	Time 0.352	Loss 2.77 	mAP 28.31
Test: [0/22]	Time 0.873 (0.873)	Precision 52.12 (52.12)	Recall 64.74 (64.74) 	 P_C 28.07 	 R_C 32.07 	 F_C 28.28 	 P_O 52.12 	 R_O 64.74 	 F_O 57.75
Test: [6/50]	  P_C 34.73 	 R_C 48.68 	 F_C 37.48 	 P_O 48.62 	 R_O 65.40 	 F_O 55.78 	 mAP 42.31
Train: [0/156]	Time 1.309 (1.309)	Loss 2.36 (2.36)	mAP 26.72 (26.72)
Train: [100/156]	Time 0.345 (0.356)	Loss 2.91 (2.72)	mAP 33.19 (28.82)
Train: [7/50]	Time 0.351	Loss 2.70 	mAP 28.93
Test: [0/22]	Time 0.875 (0.875)	Precision 54.78 (54.78)	Recall 67.89 (67.89) 	 P_C 28.81 	 R_C 32.33 	 F_C 29.06 	 P_O 54.78 	 R_O 67.89 	 F_O 60.63
Test: [7/50]	  P_C 37.51 	 R_C 48.74 	 F_C 39.55 	 P_O 50.02 	 R_O 67.79 	 F_O 57.57 	 mAP 43.13
Train: [0/156]	Time 1.288 (1.288)	Loss 2.82 (2.82)	mAP 26.78 (26.78)
Train: [100/156]	Time 0.345 (0.354)	Loss 3.07 (2.69)	mAP 27.64 (29.41)
Train: [8/50]	Time 0.351	Loss 2.67 	mAP 29.47
Test: [0/22]	Time 0.876 (0.876)	Precision 55.18 (55.18)	Recall 64.47 (64.47) 	 P_C 26.38 	 R_C 27.72 	 F_C 25.55 	 P_O 55.18 	 R_O 64.47 	 F_O 59.47
Test: [8/50]	  P_C 39.00 	 R_C 46.95 	 F_C 39.39 	 P_O 52.12 	 R_O 66.94 	 F_O 58.61 	 mAP 43.85
Train: [0/156]	Time 1.068 (1.068)	Loss 2.56 (2.56)	mAP 31.47 (31.47)
Train: [100/156]	Time 0.341 (0.354)	Loss 2.41 (2.63)	mAP 24.59 (29.54)
Train: [9/50]	Time 0.351	Loss 2.63 	mAP 29.60
Test: [0/22]	Time 0.867 (0.867)	Precision 54.91 (54.91)	Recall 67.63 (67.63) 	 P_C 29.10 	 R_C 32.63 	 F_C 29.11 	 P_O 54.91 	 R_O 67.63 	 F_O 60.61
Test: [9/50]	  P_C 37.74 	 R_C 50.97 	 F_C 40.46 	 P_O 51.08 	 R_O 68.14 	 F_O 58.39 	 mAP 45.40
Train: [0/156]	Time 1.130 (1.130)	Loss 3.02 (3.02)	mAP 35.83 (35.83)
Train: [100/156]	Time 0.341 (0.352)	Loss 2.63 (2.61)	mAP 31.71 (30.00)
Train: [10/50]	Time 0.349	Loss 2.59 	mAP 30.32
Test: [0/22]	Time 0.879 (0.879)	Precision 55.65 (55.65)	Recall 68.68 (68.68) 	 P_C 30.03 	 R_C 34.49 	 F_C 30.31 	 P_O 55.65 	 R_O 68.68 	 F_O 61.48
Test: [10/50]	  P_C 41.23 	 R_C 50.34 	 F_C 41.94 	 P_O 51.27 	 R_O 67.88 	 F_O 58.42 	 mAP 45.17
Train: [0/156]	Time 1.159 (1.159)	Loss 2.61 (2.61)	mAP 29.57 (29.57)
Train: [100/156]	Time 0.345 (0.354)	Loss 2.13 (2.60)	mAP 27.30 (30.19)
Train: [11/50]	Time 0.351	Loss 2.61 	mAP 30.02
Test: [0/22]	Time 0.885 (0.885)	Precision 56.14 (56.14)	Recall 69.74 (69.74) 	 P_C 28.92 	 R_C 34.62 	 F_C 30.14 	 P_O 56.14 	 R_O 69.74 	 F_O 62.21
Test: [11/50]	  P_C 38.41 	 R_C 52.65 	 F_C 41.46 	 P_O 51.54 	 R_O 68.72 	 F_O 58.90 	 mAP 46.08
Train: [0/156]	Time 0.987 (0.987)	Loss 2.34 (2.34)	mAP 24.09 (24.09)
Train: [100/156]	Time 0.353 (0.355)	Loss 2.51 (2.56)	mAP 26.71 (30.14)
Train: [12/50]	Time 0.352	Loss 2.57 	mAP 30.17
Test: [0/22]	Time 0.870 (0.870)	Precision 57.96 (57.96)	Recall 68.95 (68.95) 	 P_C 27.80 	 R_C 31.68 	 F_C 28.32 	 P_O 57.96 	 R_O 68.95 	 F_O 62.98
Test: [12/50]	  P_C 39.42 	 R_C 50.81 	 F_C 42.34 	 P_O 53.10 	 R_O 69.04 	 F_O 60.03 	 mAP 45.44
Train: [0/156]	Time 0.788 (0.788)	Loss 2.64 (2.64)	mAP 31.13 (31.13)
Train: [100/156]	Time 0.344 (0.350)	Loss 2.80 (2.55)	mAP 25.44 (30.39)
Train: [13/50]	Time 0.349	Loss 2.55 	mAP 30.48
Test: [0/22]	Time 0.882 (0.882)	Precision 60.14 (60.14)	Recall 67.11 (67.11) 	 P_C 30.64 	 R_C 31.95 	 F_C 29.97 	 P_O 60.14 	 R_O 67.11 	 F_O 63.43
Test: [13/50]	  P_C 41.32 	 R_C 50.69 	 F_C 42.63 	 P_O 54.66 	 R_O 68.13 	 F_O 60.66 	 mAP 46.25
Train: [0/156]	Time 1.188 (1.188)	Loss 2.74 (2.74)	mAP 29.14 (29.14)
Train: [100/156]	Time 0.342 (0.354)	Loss 2.52 (2.52)	mAP 29.32 (30.92)
Train: [14/50]	Time 0.352	Loss 2.53 	mAP 30.88
Test: [0/22]	Time 0.872 (0.872)	Precision 56.65 (56.65)	Recall 69.47 (69.47) 	 P_C 30.87 	 R_C 36.31 	 F_C 31.54 	 P_O 56.65 	 R_O 69.47 	 F_O 62.41
Test: [14/50]	  P_C 39.63 	 R_C 52.51 	 F_C 42.58 	 P_O 53.27 	 R_O 68.88 	 F_O 60.08 	 mAP 46.48
Train: [0/156]	Time 0.835 (0.835)	Loss 2.60 (2.60)	mAP 30.43 (30.43)
Train: [100/156]	Time 0.347 (0.351)	Loss 2.67 (2.49)	mAP 31.55 (31.07)
Train: [15/50]	Time 0.349	Loss 2.49 	mAP 30.84
Test: [0/22]	Time 0.876 (0.876)	Precision 60.23 (60.23)	Recall 68.16 (68.16) 	 P_C 31.51 	 R_C 33.36 	 F_C 30.47 	 P_O 60.23 	 R_O 68.16 	 F_O 63.95
Test: [15/50]	  P_C 40.78 	 R_C 51.97 	 F_C 42.86 	 P_O 54.89 	 R_O 68.21 	 F_O 60.83 	 mAP 46.97
Train: [0/156]	Time 1.401 (1.401)	Loss 2.54 (2.54)	mAP 39.04 (39.04)
Train: [100/156]	Time 0.348 (0.356)	Loss 2.28 (2.50)	mAP 28.59 (30.57)
Train: [16/50]	Time 0.352	Loss 2.50 	mAP 30.69
Test: [0/22]	Time 0.877 (0.877)	Precision 57.53 (57.53)	Recall 67.37 (67.37) 	 P_C 29.62 	 R_C 32.48 	 F_C 29.69 	 P_O 57.53 	 R_O 67.37 	 F_O 62.06
Test: [16/50]	  P_C 39.73 	 R_C 51.45 	 F_C 41.98 	 P_O 52.91 	 R_O 68.94 	 F_O 59.87 	 mAP 46.71
Train: [0/156]	Time 1.025 (1.025)	Loss 2.33 (2.33)	mAP 32.64 (32.64)
Train: [100/156]	Time 0.345 (0.354)	Loss 2.47 (2.46)	mAP 31.49 (31.46)
Train: [17/50]	Time 0.351	Loss 2.47 	mAP 31.17
Test: [0/22]	Time 0.875 (0.875)	Precision 57.65 (57.65)	Recall 68.42 (68.42) 	 P_C 30.50 	 R_C 35.85 	 F_C 31.78 	 P_O 57.65 	 R_O 68.42 	 F_O 62.58
Test: [17/50]	  P_C 39.90 	 R_C 50.84 	 F_C 42.67 	 P_O 53.10 	 R_O 68.16 	 F_O 59.69 	 mAP 46.61
Train: [0/156]	Time 0.886 (0.886)	Loss 2.71 (2.71)	mAP 32.41 (32.41)
Train: [100/156]	Time 0.347 (0.354)	Loss 2.93 (2.47)	mAP 26.95 (31.80)
Train: [18/50]	Time 0.353	Loss 2.47 	mAP 31.50
Test: [0/22]	Time 0.877 (0.877)	Precision 60.94 (60.94)	Recall 68.16 (68.16) 	 P_C 30.75 	 R_C 33.84 	 F_C 31.12 	 P_O 60.94 	 R_O 68.16 	 F_O 64.35
Test: [18/50]	  P_C 41.08 	 R_C 51.40 	 F_C 43.40 	 P_O 55.85 	 R_O 68.21 	 F_O 61.41 	 mAP 47.16
Train: [0/156]	Time 1.053 (1.053)	Loss 2.39 (2.39)	mAP 26.34 (26.34)
Train: [100/156]	Time 0.346 (0.354)	Loss 2.22 (2.45)	mAP 33.07 (30.86)
Train: [19/50]	Time 0.351	Loss 2.44 	mAP 31.21
Test: [0/22]	Time 0.887 (0.887)	Precision 58.57 (58.57)	Recall 69.21 (69.21) 	 P_C 30.85 	 R_C 33.49 	 F_C 30.74 	 P_O 58.57 	 R_O 69.21 	 F_O 63.45
Test: [19/50]	  P_C 40.42 	 R_C 52.47 	 F_C 44.03 	 P_O 54.99 	 R_O 68.88 	 F_O 61.16 	 mAP 46.99
Train: [0/156]	Time 0.937 (0.937)	Loss 2.44 (2.44)	mAP 31.98 (31.98)
Train: [100/156]	Time 0.340 (0.351)	Loss 2.35 (2.43)	mAP 33.17 (31.90)
Train: [20/50]	Time 0.349	Loss 2.43 	mAP 31.89
Test: [0/22]	Time 0.883 (0.883)	Precision 61.94 (61.94)	Recall 68.95 (68.95) 	 P_C 30.56 	 R_C 32.74 	 F_C 30.51 	 P_O 61.94 	 R_O 68.95 	 F_O 65.26
Test: [20/50]	  P_C 41.47 	 R_C 51.99 	 F_C 43.89 	 P_O 56.58 	 R_O 68.33 	 F_O 61.90 	 mAP 47.51
Train: [0/156]	Time 0.858 (0.858)	Loss 1.99 (1.99)	mAP 33.12 (33.12)
Train: [100/156]	Time 0.343 (0.352)	Loss 2.56 (2.43)	mAP 33.02 (31.61)
Train: [21/50]	Time 0.349	Loss 2.44 	mAP 31.45
Test: [0/22]	Time 0.883 (0.883)	Precision 60.67 (60.67)	Recall 71.05 (71.05) 	 P_C 32.00 	 R_C 36.09 	 F_C 32.47 	 P_O 60.67 	 R_O 71.05 	 F_O 65.45
Test: [21/50]	  P_C 42.22 	 R_C 51.88 	 F_C 44.47 	 P_O 55.69 	 R_O 68.83 	 F_O 61.57 	 mAP 47.40
Train: [0/156]	Time 0.799 (0.799)	Loss 2.34 (2.34)	mAP 38.14 (38.14)
Train: [100/156]	Time 0.350 (0.349)	Loss 2.62 (2.39)	mAP 28.99 (31.77)
Train: [22/50]	Time 0.348	Loss 2.40 	mAP 31.81
Test: [0/22]	Time 0.880 (0.880)	Precision 61.34 (61.34)	Recall 69.74 (69.74) 	 P_C 30.58 	 R_C 32.63 	 F_C 30.25 	 P_O 61.34 	 R_O 69.74 	 F_O 65.27
Test: [22/50]	  P_C 41.83 	 R_C 52.22 	 F_C 44.35 	 P_O 55.24 	 R_O 69.13 	 F_O 61.41 	 mAP 47.78
Train: [0/156]	Time 1.004 (1.004)	Loss 2.40 (2.40)	mAP 30.58 (30.58)
Train: [100/156]	Time 0.343 (0.353)	Loss 2.24 (2.39)	mAP 29.99 (31.99)
Train: [23/50]	Time 0.351	Loss 2.42 	mAP 31.86
Test: [0/22]	Time 0.884 (0.884)	Precision 59.42 (59.42)	Recall 69.74 (69.74) 	 P_C 31.34 	 R_C 34.01 	 F_C 31.47 	 P_O 59.42 	 R_O 69.74 	 F_O 64.16
Test: [23/50]	  P_C 41.14 	 R_C 50.68 	 F_C 43.60 	 P_O 55.24 	 R_O 69.34 	 F_O 61.49 	 mAP 47.56
Train: [0/156]	Time 1.013 (1.013)	Loss 2.46 (2.46)	mAP 29.78 (29.78)
Train: [100/156]	Time 0.342 (0.354)	Loss 2.58 (2.39)	mAP 30.17 (31.55)
Train: [24/50]	Time 0.351	Loss 2.42 	mAP 31.46
Test: [0/22]	Time 0.882 (0.882)	Precision 59.82 (59.82)	Recall 69.74 (69.74) 	 P_C 32.04 	 R_C 34.47 	 F_C 31.71 	 P_O 59.82 	 R_O 69.74 	 F_O 64.40
Test: [24/50]	  P_C 42.43 	 R_C 52.85 	 F_C 45.00 	 P_O 55.61 	 R_O 69.01 	 F_O 61.59 	 mAP 47.86
Train: [0/156]	Time 0.967 (0.967)	Loss 2.35 (2.35)	mAP 28.25 (28.25)
Train: [100/156]	Time 0.343 (0.351)	Loss 2.38 (2.32)	mAP 27.33 (32.28)
Train: [25/50]	Time 0.349	Loss 2.37 	mAP 32.17
Test: [0/22]	Time 0.880 (0.880)	Precision 59.86 (59.86)	Recall 68.68 (68.68) 	 P_C 31.07 	 R_C 34.10 	 F_C 31.30 	 P_O 59.86 	 R_O 68.68 	 F_O 63.97
Test: [25/50]	  P_C 40.37 	 R_C 53.43 	 F_C 44.31 	 P_O 55.63 	 R_O 68.99 	 F_O 61.59 	 mAP 47.80
Train: [0/156]	Time 0.934 (0.934)	Loss 2.21 (2.21)	mAP 32.72 (32.72)
Train: [100/156]	Time 0.346 (0.355)	Loss 2.53 (2.38)	mAP 28.67 (32.26)
Train: [26/50]	Time 0.351	Loss 2.39 	mAP 31.96
Test: [0/22]	Time 0.881 (0.881)	Precision 60.86 (60.86)	Recall 70.79 (70.79) 	 P_C 32.33 	 R_C 34.60 	 F_C 32.41 	 P_O 60.86 	 R_O 70.79 	 F_O 65.45
Test: [26/50]	  P_C 42.94 	 R_C 51.86 	 F_C 44.86 	 P_O 55.79 	 R_O 69.49 	 F_O 61.89 	 mAP 47.89
Train: [0/156]	Time 0.922 (0.922)	Loss 2.39 (2.39)	mAP 39.83 (39.83)
Train: [100/156]	Time 0.343 (0.353)	Loss 2.83 (2.37)	mAP 31.66 (32.17)
Train: [27/50]	Time 0.351	Loss 2.37 	mAP 32.19
Test: [0/22]	Time 0.885 (0.885)	Precision 61.40 (61.40)	Recall 71.58 (71.58) 	 P_C 33.48 	 R_C 36.09 	 F_C 33.07 	 P_O 61.40 	 R_O 71.58 	 F_O 66.10
Test: [27/50]	  P_C 43.19 	 R_C 52.47 	 F_C 45.69 	 P_O 56.47 	 R_O 69.16 	 F_O 62.17 	 mAP 48.07
Train: [0/156]	Time 0.829 (0.829)	Loss 2.11 (2.11)	mAP 28.46 (28.46)
Train: [100/156]	Time 0.342 (0.350)	Loss 2.62 (2.36)	mAP 28.34 (32.37)
Train: [28/50]	Time 0.348	Loss 2.35 	mAP 32.19
Test: [0/22]	Time 0.875 (0.875)	Precision 60.79 (60.79)	Recall 68.95 (68.95) 	 P_C 32.16 	 R_C 35.09 	 F_C 32.02 	 P_O 60.79 	 R_O 68.95 	 F_O 64.61
Test: [28/50]	  P_C 41.36 	 R_C 52.26 	 F_C 44.58 	 P_O 56.85 	 R_O 68.97 	 F_O 62.33 	 mAP 48.12
Train: [0/156]	Time 0.829 (0.829)	Loss 2.15 (2.15)	mAP 31.45 (31.45)
Train: [100/156]	Time 0.342 (0.353)	Loss 2.18 (2.35)	mAP 29.05 (32.12)
Train: [29/50]	Time 0.350	Loss 2.36 	mAP 32.00
Test: [0/22]	Time 0.876 (0.876)	Precision 60.65 (60.65)	Recall 68.95 (68.95) 	 P_C 31.16 	 R_C 33.37 	 F_C 30.88 	 P_O 60.65 	 R_O 68.95 	 F_O 64.53
Test: [29/50]	  P_C 42.08 	 R_C 52.58 	 F_C 44.86 	 P_O 56.99 	 R_O 68.96 	 F_O 62.41 	 mAP 48.29
Train: [0/156]	Time 0.863 (0.863)	Loss 2.36 (2.36)	mAP 26.91 (26.91)
Train: [100/156]	Time 0.352 (0.350)	Loss 2.23 (2.35)	mAP 31.23 (31.90)
Train: [30/50]	Time 0.348	Loss 2.37 	mAP 31.96
Test: [0/22]	Time 0.873 (0.873)	Precision 61.56 (61.56)	Recall 68.68 (68.68) 	 P_C 32.06 	 R_C 33.95 	 F_C 31.78 	 P_O 61.56 	 R_O 68.68 	 F_O 64.93
Test: [30/50]	  P_C 42.82 	 R_C 52.41 	 F_C 45.39 	 P_O 57.56 	 R_O 69.03 	 F_O 62.77 	 mAP 48.45
Train: [0/156]	Time 0.842 (0.842)	Loss 2.47 (2.47)	mAP 32.75 (32.75)
Train: [100/156]	Time 0.348 (0.352)	Loss 2.27 (2.31)	mAP 32.46 (32.23)
Train: [31/50]	Time 0.351	Loss 2.31 	mAP 32.38
Test: [0/22]	Time 0.888 (0.888)	Precision 60.68 (60.68)	Recall 70.26 (70.26) 	 P_C 34.03 	 R_C 35.54 	 F_C 33.44 	 P_O 60.68 	 R_O 70.26 	 F_O 65.12
Test: [31/50]	  P_C 41.34 	 R_C 53.87 	 F_C 45.48 	 P_O 56.13 	 R_O 69.59 	 F_O 62.14 	 mAP 48.58
Train: [0/156]	Time 0.920 (0.920)	Loss 2.25 (2.25)	mAP 33.75 (33.75)
Train: [100/156]	Time 0.343 (0.351)	Loss 2.54 (2.32)	mAP 29.36 (33.22)
Train: [32/50]	Time 0.349	Loss 2.33 	mAP 32.82
Test: [0/22]	Time 0.884 (0.884)	Precision 63.03 (63.03)	Recall 70.00 (70.00) 	 P_C 32.00 	 R_C 34.55 	 F_C 32.19 	 P_O 63.03 	 R_O 70.00 	 F_O 66.33
Test: [32/50]	  P_C 43.32 	 R_C 51.69 	 F_C 45.47 	 P_O 58.44 	 R_O 68.88 	 F_O 63.23 	 mAP 48.41
Train: [0/156]	Time 0.805 (0.805)	Loss 2.48 (2.48)	mAP 30.54 (30.54)
Train: [100/156]	Time 0.345 (0.349)	Loss 2.17 (2.30)	mAP 29.11 (32.70)
Train: [33/50]	Time 0.350	Loss 2.32 	mAP 32.77
Test: [0/22]	Time 0.897 (0.897)	Precision 60.82 (60.82)	Recall 70.26 (70.26) 	 P_C 31.64 	 R_C 34.69 	 F_C 32.13 	 P_O 60.82 	 R_O 70.26 	 F_O 65.20
Test: [33/50]	  P_C 41.82 	 R_C 52.78 	 F_C 45.19 	 P_O 57.14 	 R_O 68.92 	 F_O 62.48 	 mAP 48.38
Train: [0/156]	Time 1.115 (1.115)	Loss 1.92 (1.92)	mAP 32.55 (32.55)
Train: [100/156]	Time 0.410 (0.359)	Loss 2.19 (2.30)	mAP 30.95 (32.85)
Train: [34/50]	Time 0.355	Loss 2.30 	mAP 32.67
Test: [0/22]	Time 0.883 (0.883)	Precision 61.28 (61.28)	Recall 70.79 (70.79) 	 P_C 32.17 	 R_C 35.01 	 F_C 32.40 	 P_O 61.28 	 R_O 70.79 	 F_O 65.69
Test: [34/50]	  P_C 42.94 	 R_C 52.53 	 F_C 45.87 	 P_O 57.88 	 R_O 69.16 	 F_O 63.02 	 mAP 48.41
Train: [0/156]	Time 1.433 (1.433)	Loss 2.39 (2.39)	mAP 31.36 (31.36)
Train: [100/156]	Time 0.344 (0.356)	Loss 2.45 (2.30)	mAP 29.73 (32.82)
Train: [35/50]	Time 0.353	Loss 2.30 	mAP 32.91
Test: [0/22]	Time 0.874 (0.874)	Precision 61.50 (61.50)	Recall 68.95 (68.95) 	 P_C 32.76 	 R_C 34.23 	 F_C 32.20 	 P_O 61.50 	 R_O 68.95 	 F_O 65.01
Test: [35/50]	  P_C 44.00 	 R_C 51.47 	 F_C 45.79 	 P_O 58.50 	 R_O 68.74 	 F_O 63.21 	 mAP 48.38
Train: [0/156]	Time 1.124 (1.124)	Loss 2.25 (2.25)	mAP 30.29 (30.29)
Train: [100/156]	Time 0.347 (0.357)	Loss 2.06 (2.29)	mAP 30.51 (32.51)
Train: [36/50]	Time 0.354	Loss 2.30 	mAP 32.72
Test: [0/22]	Time 0.891 (0.891)	Precision 60.27 (60.27)	Recall 70.26 (70.26) 	 P_C 31.84 	 R_C 34.75 	 F_C 32.12 	 P_O 60.27 	 R_O 70.26 	 F_O 64.88
Test: [36/50]	  P_C 42.71 	 R_C 53.18 	 F_C 45.99 	 P_O 57.26 	 R_O 69.39 	 F_O 62.74 	 mAP 48.70
Train: [0/156]	Time 1.190 (1.190)	Loss 2.30 (2.30)	mAP 28.16 (28.16)
Train: [100/156]	Time 0.344 (0.352)	Loss 2.63 (2.26)	mAP 30.28 (33.12)
Train: [37/50]	Time 0.350	Loss 2.28 	mAP 33.06
Test: [0/22]	Time 0.880 (0.880)	Precision 60.55 (60.55)	Recall 69.47 (69.47) 	 P_C 31.26 	 R_C 34.36 	 F_C 31.45 	 P_O 60.55 	 R_O 69.47 	 F_O 64.71
Test: [37/50]	  P_C 43.60 	 R_C 52.22 	 F_C 45.96 	 P_O 58.47 	 R_O 69.16 	 F_O 63.37 	 mAP 48.60
Train: [0/156]	Time 0.948 (0.948)	Loss 2.18 (2.18)	mAP 29.29 (29.29)
Train: [100/156]	Time 0.347 (0.351)	Loss 2.13 (2.27)	mAP 36.91 (33.28)
Train: [38/50]	Time 0.348	Loss 2.27 	mAP 33.23
Test: [0/22]	Time 0.892 (0.892)	Precision 62.00 (62.00)	Recall 70.00 (70.00) 	 P_C 32.55 	 R_C 34.67 	 F_C 32.49 	 P_O 62.00 	 R_O 70.00 	 F_O 65.76
Test: [38/50]	  P_C 43.63 	 R_C 52.78 	 F_C 46.21 	 P_O 58.26 	 R_O 68.90 	 F_O 63.13 	 mAP 48.95
Train: [0/156]	Time 1.184 (1.184)	Loss 2.45 (2.45)	mAP 32.70 (32.70)
Train: [100/156]	Time 0.346 (0.355)	Loss 2.12 (2.28)	mAP 38.94 (33.11)
Train: [39/50]	Time 0.353	Loss 2.27 	mAP 32.90
Test: [0/22]	Time 0.888 (0.888)	Precision 59.82 (59.82)	Recall 70.53 (70.53) 	 P_C 30.43 	 R_C 34.29 	 F_C 30.97 	 P_O 59.82 	 R_O 70.53 	 F_O 64.73
Test: [39/50]	  P_C 42.73 	 R_C 52.80 	 F_C 45.83 	 P_O 57.43 	 R_O 69.75 	 F_O 62.99 	 mAP 48.94
Train: [0/156]	Time 0.761 (0.761)	Loss 2.47 (2.47)	mAP 33.75 (33.75)
Train: [100/156]	Time 0.344 (0.348)	Loss 2.32 (2.23)	mAP 41.58 (33.08)
Train: [40/50]	Time 0.347	Loss 2.24 	mAP 33.15
Test: [0/22]	Time 0.881 (0.881)	Precision 62.23 (62.23)	Recall 68.95 (68.95) 	 P_C 32.50 	 R_C 34.34 	 F_C 32.36 	 P_O 62.23 	 R_O 68.95 	 F_O 65.42
Test: [40/50]	  P_C 43.36 	 R_C 53.26 	 F_C 46.39 	 P_O 58.81 	 R_O 69.05 	 F_O 63.52 	 mAP 48.97
Train: [0/156]	Time 0.861 (0.861)	Loss 2.10 (2.10)	mAP 31.63 (31.63)
Train: [100/156]	Time 0.346 (0.352)	Loss 2.61 (2.27)	mAP 35.83 (33.11)
Train: [41/50]	Time 0.350	Loss 2.26 	mAP 33.26
Test: [0/22]	Time 0.886 (0.886)	Precision 63.15 (63.15)	Recall 70.79 (70.79) 	 P_C 33.06 	 R_C 34.83 	 F_C 32.83 	 P_O 63.15 	 R_O 70.79 	 F_O 66.75
Test: [41/50]	  P_C 43.82 	 R_C 53.15 	 F_C 46.57 	 P_O 58.67 	 R_O 69.20 	 F_O 63.50 	 mAP 48.95
Train: [0/156]	Time 1.001 (1.001)	Loss 2.20 (2.20)	mAP 28.39 (28.39)
Train: [100/156]	Time 0.344 (0.351)	Loss 2.41 (2.27)	mAP 33.98 (32.83)
Train: [42/50]	Time 0.348	Loss 2.27 	mAP 32.88
Test: [0/22]	Time 0.891 (0.891)	Precision 61.63 (61.63)	Recall 69.74 (69.74) 	 P_C 32.43 	 R_C 34.49 	 F_C 32.22 	 P_O 61.63 	 R_O 69.74 	 F_O 65.43
Test: [42/50]	  P_C 43.65 	 R_C 52.55 	 F_C 46.31 	 P_O 58.97 	 R_O 69.05 	 F_O 63.61 	 mAP 48.82
Train: [0/156]	Time 0.837 (0.837)	Loss 1.94 (1.94)	mAP 27.15 (27.15)
Train: [100/156]	Time 0.345 (0.352)	Loss 2.88 (2.25)	mAP 28.52 (32.97)
Train: [43/50]	Time 0.350	Loss 2.25 	mAP 33.13
Test: [0/22]	Time 0.885 (0.885)	Precision 62.24 (62.24)	Recall 70.26 (70.26) 	 P_C 31.85 	 R_C 34.61 	 F_C 32.17 	 P_O 62.24 	 R_O 70.26 	 F_O 66.01
Test: [43/50]	  P_C 44.07 	 R_C 52.42 	 F_C 46.56 	 P_O 58.99 	 R_O 68.99 	 F_O 63.60 	 mAP 48.80
Train: [0/156]	Time 0.961 (0.961)	Loss 2.50 (2.50)	mAP 26.15 (26.15)
Train: [100/156]	Time 0.347 (0.356)	Loss 2.42 (2.26)	mAP 32.10 (33.40)
Train: [44/50]	Time 0.352	Loss 2.26 	mAP 33.32
Test: [0/22]	Time 0.882 (0.882)	Precision 62.95 (62.95)	Recall 69.74 (69.74) 	 P_C 32.46 	 R_C 34.50 	 F_C 32.26 	 P_O 62.95 	 R_O 69.74 	 F_O 66.17
Test: [44/50]	  P_C 44.72 	 R_C 52.61 	 F_C 46.96 	 P_O 59.54 	 R_O 68.87 	 F_O 63.87 	 mAP 48.84
Train: [0/156]	Time 1.131 (1.131)	Loss 2.08 (2.08)	mAP 32.43 (32.43)
Train: [100/156]	Time 0.346 (0.355)	Loss 2.36 (2.24)	mAP 29.71 (32.87)
Train: [45/50]	Time 0.355	Loss 2.25 	mAP 32.80
Test: [0/22]	Time 0.899 (0.899)	Precision 62.30 (62.30)	Recall 70.00 (70.00) 	 P_C 32.40 	 R_C 34.60 	 F_C 32.22 	 P_O 62.30 	 R_O 70.00 	 F_O 65.92
Test: [45/50]	  P_C 44.55 	 R_C 52.89 	 F_C 46.95 	 P_O 59.10 	 R_O 69.29 	 F_O 63.79 	 mAP 48.88
Train: [0/156]	Time 0.994 (0.994)	Loss 2.33 (2.33)	mAP 36.64 (36.64)
Train: [100/156]	Time 0.348 (0.354)	Loss 2.63 (2.26)	mAP 23.53 (33.38)
Train: [46/50]	Time 0.351	Loss 2.24 	mAP 33.32
Test: [0/22]	Time 0.892 (0.892)	Precision 61.92 (61.92)	Recall 69.74 (69.74) 	 P_C 32.45 	 R_C 34.56 	 F_C 32.27 	 P_O 61.92 	 R_O 69.74 	 F_O 65.59
Test: [46/50]	  P_C 44.40 	 R_C 52.34 	 F_C 46.64 	 P_O 59.13 	 R_O 69.05 	 F_O 63.71 	 mAP 48.89
Train: [0/156]	Time 1.058 (1.058)	Loss 2.04 (2.04)	mAP 37.18 (37.18)
Train: [100/156]	Time 0.353 (0.355)	Loss 2.59 (2.23)	mAP 30.58 (33.11)
Train: [47/50]	Time 0.353	Loss 2.25 	mAP 33.16
Test: [0/22]	Time 0.906 (0.906)	Precision 62.00 (62.00)	Recall 70.00 (70.00) 	 P_C 32.47 	 R_C 34.60 	 F_C 32.33 	 P_O 62.00 	 R_O 70.00 	 F_O 65.76
Test: [47/50]	  P_C 44.34 	 R_C 52.73 	 F_C 46.78 	 P_O 58.99 	 R_O 69.30 	 F_O 63.73 	 mAP 48.90
Train: [0/156]	Time 0.993 (0.993)	Loss 2.30 (2.30)	mAP 34.43 (34.43)
Train: [100/156]	Time 0.341 (0.357)	Loss 2.03 (2.24)	mAP 31.29 (33.00)
Train: [48/50]	Time 0.354	Loss 2.25 	mAP 32.98
Test: [0/22]	Time 0.887 (0.887)	Precision 62.15 (62.15)	Recall 70.00 (70.00) 	 P_C 32.99 	 R_C 34.63 	 F_C 32.53 	 P_O 62.15 	 R_O 70.00 	 F_O 65.84
Test: [48/50]	  P_C 44.27 	 R_C 52.71 	 F_C 46.80 	 P_O 59.10 	 R_O 69.21 	 F_O 63.76 	 mAP 48.97
Train: [0/156]	Time 1.229 (1.229)	Loss 2.30 (2.30)	mAP 31.78 (31.78)
Train: [100/156]	Time 0.344 (0.354)	Loss 2.14 (2.26)	mAP 31.01 (33.65)
Train: [49/50]	Time 0.352	Loss 2.23 	mAP 33.33
Test: [0/22]	Time 0.905 (0.905)	Precision 62.38 (62.38)	Recall 70.26 (70.26) 	 P_C 32.51 	 R_C 34.68 	 F_C 32.40 	 P_O 62.38 	 R_O 70.26 	 F_O 66.09
Test: [49/50]	  P_C 44.46 	 R_C 52.73 	 F_C 46.88 	 P_O 59.08 	 R_O 69.22 	 F_O 63.75 	 mAP 48.96
Train: [0/156]	Time 1.180 (1.180)	Loss 2.57 (2.57)	mAP 33.96 (33.96)
Train: [100/156]	Time 0.348 (0.360)	Loss 2.41 (2.24)	mAP 30.33 (33.51)
Train: [50/50]	Time 0.356	Loss 2.22 	mAP 33.42
Test: [0/22]	Time 0.886 (0.886)	Precision 62.24 (62.24)	Recall 70.26 (70.26) 	 P_C 32.51 	 R_C 34.68 	 F_C 32.40 	 P_O 62.24 	 R_O 70.26 	 F_O 66.01
Test: [50/50]	  P_C 44.48 	 R_C 52.70 	 F_C 46.87 	 P_O 59.11 	 R_O 69.18 	 F_O 63.75 	 mAP 48.97
Evaluating the best model
Evaluate with threshold 0.50
... loading pretrained weights from ./output/foodseg103-RN101-cosine-bs32-e50/model_best.pth.tar
Test: [0/22]	Time 0.888 (0.888)	Precision 62.24 (62.24)	Recall 70.26 (70.26) 	 P_C 32.51 	 R_C 34.68 	 F_C 32.40 	 P_O 62.24 	 R_O 70.26 	 F_O 66.01
Test: [50/50]	  P_C 44.48 	 R_C 52.70 	 F_C 46.87 	 P_O 59.11 	 R_O 69.18 	 F_O 63.75 	 mAP 48.97

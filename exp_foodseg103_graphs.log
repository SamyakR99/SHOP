nohup: ignoring input
 -------------------- Building Dataset ----------------------
DATASET.ROOT = /home/samyakr2/food/FoodSeg103/Images
data_split = trainval
PARTIAL_PORTION= 1.000000
INPUT.SIZE = 224
 -------------------- Building Dataset ----------------------
DATASET.ROOT = /home/samyakr2/food/FoodSeg103/Images
data_split = test
PARTIAL_PORTION= 1.000000
INPUT.SIZE = 224
 -------------------- Building Dataset ----------------------
DATASET.ROOT = /home/samyakr2/food/FoodSeg103/Images
data_split = test
PARTIAL_PORTION= 1.000000
INPUT.SIZE = 224
Loading CLIP (backbone: RN101)
Building dualcoop
Initializing class-specific contexts
Initial positive context: "X X X X X X X X X X X X X X X X"
Initial negative  context: "X X X X X X X X X X X X X X X X"
Number of positive context words (tokens): 16
Number of negative context words (tokens): 16
Freeze the backbone weights
Freeze the attn weights
image_encoder.attnpool.positional_embedding
image_encoder.attnpool.k_proj.weight
image_encoder.attnpool.k_proj.bias
image_encoder.attnpool.q_proj.weight
image_encoder.attnpool.q_proj.bias
image_encoder.attnpool.v_proj.weight
image_encoder.attnpool.v_proj.bias
image_encoder.attnpool.c_proj.weight
image_encoder.attnpool.c_proj.bias
Multiple GPUs detected (n_gpus=2), use all of them!
num of params in prompt learner:  2
train.py --config_file configs/models/rn101_ep50.yaml --datadir /home/samyakr2/food/FoodSeg103/Images --dataset_config_file configs/datasets/foodseg103.yaml --input_size 224 --lr 0.001 --loss_w 0.03 -pp 1 --csc
Namespace(prefix='', resume=None, pretrained=None, auto_resume=False, datadir='/home/samyakr2/food/FoodSeg103/Images', input_size=224, train_input_size=None, num_train_cls=100, test_input_size=None, thre=0.5, single_prompt='pos', output_dir='', print_freq=100, val_freq_in_epoch=-1, evaluate=False, config_file='configs/models/rn101_ep50.yaml', dataset_config_file='configs/datasets/foodseg103.yaml', positive_prompt=None, negative_prompt=None, n_ctx_pos=None, n_ctx_neg=None, lr=0.001, loss_w=0.03, csc=True, logit_scale=100.0, gamma_neg=2.0, gamma_pos=1.0, portion=1.0, partial_portion=1.0, mask_file=None, train_batch_size=None, stop_epochs=None, max_epochs=None, finetune=False, finetune_backbone=False, finetune_attn=False, finetune_text=False, base_lr_mult=None, backbone_lr_mult=None, text_lr_mult=None, attn_lr_mult=None, val_every_n_epochs=1, warmup_epochs=1, top_k=3)
DataParallel(
  (module): DualCoop(
    (prompt_learner): MLCPromptLearner()
    (image_encoder): ModifiedResNet_conv_proj(
      (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
      (relu): ReLU(inplace=True)
      (layer1): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (-1): AvgPool2d(kernel_size=1, stride=1, padding=0)
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (layer2): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (layer3): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (6): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (7): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (8): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (9): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (10): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (11): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (12): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (13): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (14): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (15): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (16): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (17): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (18): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (19): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (20): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (21): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (22): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (layer4): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (attnpool): AttentionConv(
        (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
        (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
        (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
        (c_proj): Linear(in_features=2048, out_features=512, bias=True)
      )
    )
    (text_encoder): TextEncoder(
      (transformer): Transformer(
        (resblocks): Sequential(
          (0): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (1): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (2): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (3): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (4): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (5): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (6): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (7): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (8): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (9): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (10): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (11): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (gc1): GraphConvolution (512 -> 1024)
    (gc2): GraphConvolution (1024 -> 1024)
    (gc3): GraphConvolution (1024 -> 512)
    (relu): LeakyReLU(negative_slope=0.2)
    (relu2): LeakyReLU(negative_slope=0.2)
  )
)
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 3
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
    SHUFFLE: False
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    PARTIAL_PORTION: 1.0
    PORTION: 1.0
    SAMPLER: RandomSampler
    SHUFFLE: True
  VAL:
    BATCH_SIZE: 100
    SHUFFLE: False
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  MASK_FILE: None
  NAME: foodseg103
  NUM_LABELED: -1
  NUM_SHOTS: -1
  ROOT: /home/samyakr2/food/FoodSeg103/Images
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  TARGET_DOMAINS: ()
  TEST_GZSL_SPLIT: 
  TEST_SPLIT: test
  TRAIN_SPLIT: trainval
  VAL_GZSL_SPLIT: 
  VAL_PERCENT: 0.1
  VAL_SPLIT: test
  ZS_TEST: 
  ZS_TEST_UNSEEN: 
  ZS_TRAIN: 
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TEST:
    SIZE: (224, 224)
  TRAIN:
    SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MLCCLIP:
  FLOAT: False
  NEGATIVE_PROMPT: 
  POSITIVE_PROMPT: 
MODEL:
  BACKBONE:
    NAME: RN101
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  ATTN_LR_MULT: 0.1
  BACKBONE_LR_MULT: 0.1
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.001
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: ./output
RESUME: 
SEED: -1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 100
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COOP_MLC:
    ASL_GAMMA_NEG: 2.0
    ASL_GAMMA_POS: 1.0
    CSC: True
    LS: 100.0
    NEGATIVE_PROMPT_INIT: 
    N_CTX_NEG: 16
    N_CTX_POS: 16
    POSITIVE_PROMPT_INIT: 
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FINETUNE: False
  FINETUNE_ATTN: False
  FINETUNE_BACKBONE: False
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: 
  RESNET_IMAGENET:
    DEPTH: 50
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
/home/samyakr2/anaconda3/lib/python3.11/site-packages/torch/_tensor.py:836: UserWarning: non-inplace resize is deprecated
  warnings.warn("non-inplace resize is deprecated")
Train: [0/156]	Time 3.337 (3.337)	Loss 64.81 (64.81)	mAP 7.99 (7.99)
Train: [100/156]	Time 0.413 (0.380)	Loss 4.91 (9.08)	mAP 14.04 (9.89)
Train: [1/50]	Time 0.369	Loss 7.70 	mAP 10.99
Test: [0/22]	Time 0.936 (0.936)	Precision 21.69 (21.69)	Recall 53.95 (53.95) 	 P_C 8.98 	 R_C 20.11 	 F_C 11.95 	 P_O 21.69 	 R_O 53.95 	 F_O 30.94
Test: [1/50]	  P_C 8.81 	 R_C 23.36 	 F_C 12.24 	 P_O 18.32 	 R_O 49.36 	 F_O 26.72 	 mAP 11.93
Train: [0/156]	Time 1.109 (1.109)	Loss 5.27 (5.27)	mAP 12.92 (12.92)
Train: [100/156]	Time 0.348 (0.359)	Loss 3.41 (4.87)	mAP 24.59 (19.77)
Train: [2/50]	Time 0.355	Loss 4.39 	mAP 20.84
Test: [0/22]	Time 0.886 (0.886)	Precision 45.49 (45.49)	Recall 63.68 (63.68) 	 P_C 24.66 	 R_C 30.05 	 F_C 24.66 	 P_O 45.49 	 R_O 63.68 	 F_O 53.07
Test: [2/50]	  P_C 28.27 	 R_C 40.47 	 F_C 29.18 	 P_O 40.64 	 R_O 60.02 	 F_O 48.46 	 mAP 34.35
Train: [0/156]	Time 0.798 (0.798)	Loss 2.81 (2.81)	mAP 25.24 (25.24)
Train: [100/156]	Time 0.348 (0.355)	Loss 2.95 (3.14)	mAP 32.35 (25.65)
Train: [3/50]	Time 0.353	Loss 3.11 	mAP 25.56
Test: [0/22]	Time 0.882 (0.882)	Precision 51.33 (51.33)	Recall 65.79 (65.79) 	 P_C 25.20 	 R_C 31.20 	 F_C 26.47 	 P_O 51.33 	 R_O 65.79 	 F_O 57.67
Test: [3/50]	  P_C 32.66 	 R_C 44.10 	 F_C 34.33 	 P_O 46.85 	 R_O 63.65 	 F_O 53.97 	 mAP 38.99
Train: [0/156]	Time 0.821 (0.821)	Loss 2.79 (2.79)	mAP 27.27 (27.27)
Train: [100/156]	Time 0.344 (0.354)	Loss 3.27 (2.96)	mAP 26.10 (26.91)
Train: [4/50]	Time 0.354	Loss 2.93 	mAP 26.90
Test: [0/22]	Time 0.890 (0.890)	Precision 53.10 (53.10)	Recall 67.63 (67.63) 	 P_C 29.77 	 R_C 31.90 	 F_C 28.91 	 P_O 53.10 	 R_O 67.63 	 F_O 59.49
Test: [4/50]	  P_C 35.29 	 R_C 44.89 	 F_C 35.98 	 P_O 47.20 	 R_O 65.05 	 F_O 54.71 	 mAP 40.58
Train: [0/156]	Time 1.060 (1.060)	Loss 2.73 (2.73)	mAP 34.32 (34.32)
Train: [100/156]	Time 0.348 (0.361)	Loss 2.48 (2.83)	mAP 23.75 (28.14)
Train: [5/50]	Time 0.356	Loss 2.82 	mAP 27.76
Test: [0/22]	Time 0.880 (0.880)	Precision 55.02 (55.02)	Recall 66.32 (66.32) 	 P_C 28.20 	 R_C 31.02 	 F_C 28.26 	 P_O 55.02 	 R_O 66.32 	 F_O 60.14
Test: [5/50]	  P_C 36.42 	 R_C 44.54 	 F_C 37.13 	 P_O 51.78 	 R_O 63.91 	 F_O 57.21 	 mAP 41.47
Train: [0/156]	Time 0.804 (0.804)	Loss 2.95 (2.95)	mAP 28.74 (28.74)
Train: [100/156]	Time 0.345 (0.353)	Loss 2.66 (2.74)	mAP 30.08 (28.92)
Train: [6/50]	Time 0.352	Loss 2.73 	mAP 28.73
Test: [0/22]	Time 0.885 (0.885)	Precision 53.49 (53.49)	Recall 66.58 (66.58) 	 P_C 25.84 	 R_C 32.32 	 F_C 27.33 	 P_O 53.49 	 R_O 66.58 	 F_O 59.32
Test: [6/50]	  P_C 33.37 	 R_C 48.51 	 F_C 36.76 	 P_O 49.76 	 R_O 65.53 	 F_O 56.57 	 mAP 42.46
Train: [0/156]	Time 1.138 (1.138)	Loss 2.61 (2.61)	mAP 29.68 (29.68)
Train: [100/156]	Time 0.343 (0.357)	Loss 2.90 (2.70)	mAP 26.38 (28.89)
Train: [7/50]	Time 0.354	Loss 2.69 	mAP 29.01
Test: [0/22]	Time 0.885 (0.885)	Precision 55.75 (55.75)	Recall 66.32 (66.32) 	 P_C 29.50 	 R_C 30.82 	 F_C 28.16 	 P_O 55.75 	 R_O 66.32 	 F_O 60.58
Test: [7/50]	  P_C 37.87 	 R_C 47.17 	 F_C 39.55 	 P_O 52.41 	 R_O 65.35 	 F_O 58.17 	 mAP 43.63
Train: [0/156]	Time 1.029 (1.029)	Loss 2.95 (2.95)	mAP 25.76 (25.76)
Train: [100/156]	Time 0.343 (0.354)	Loss 2.61 (2.65)	mAP 27.28 (29.73)
Train: [8/50]	Time 0.353	Loss 2.66 	mAP 29.50
Test: [0/22]	Time 0.883 (0.883)	Precision 55.17 (55.17)	Recall 67.37 (67.37) 	 P_C 28.96 	 R_C 32.03 	 F_C 28.91 	 P_O 55.17 	 R_O 67.37 	 F_O 60.66
Test: [8/50]	  P_C 38.75 	 R_C 48.54 	 F_C 40.56 	 P_O 51.20 	 R_O 67.38 	 F_O 58.19 	 mAP 43.88
Train: [0/156]	Time 1.098 (1.098)	Loss 2.25 (2.25)	mAP 34.79 (34.79)
Train: [100/156]	Time 0.345 (0.357)	Loss 2.33 (2.63)	mAP 32.77 (29.87)
Train: [9/50]	Time 0.353	Loss 2.63 	mAP 29.78
Test: [0/22]	Time 0.891 (0.891)	Precision 55.46 (55.46)	Recall 66.84 (66.84) 	 P_C 28.55 	 R_C 32.14 	 F_C 28.56 	 P_O 55.46 	 R_O 66.84 	 F_O 60.62
Test: [9/50]	  P_C 37.93 	 R_C 48.95 	 F_C 39.73 	 P_O 52.99 	 R_O 66.39 	 F_O 58.94 	 mAP 44.61
Train: [0/156]	Time 0.893 (0.893)	Loss 2.59 (2.59)	mAP 25.56 (25.56)
Train: [100/156]	Time 0.345 (0.353)	Loss 2.90 (2.59)	mAP 27.27 (30.28)
Train: [10/50]	Time 0.351	Loss 2.61 	mAP 30.03
Test: [0/22]	Time 0.909 (0.909)	Precision 56.47 (56.47)	Recall 66.58 (66.58) 	 P_C 28.64 	 R_C 31.73 	 F_C 28.53 	 P_O 56.47 	 R_O 66.58 	 F_O 61.11
Test: [10/50]	  P_C 40.35 	 R_C 48.24 	 F_C 40.71 	 P_O 53.47 	 R_O 66.88 	 F_O 59.43 	 mAP 44.86
Train: [0/156]	Time 0.904 (0.904)	Loss 2.45 (2.45)	mAP 28.23 (28.23)
Train: [100/156]	Time 0.347 (0.355)	Loss 2.51 (2.57)	mAP 27.18 (30.31)
Train: [11/50]	Time 0.355	Loss 2.56 	mAP 30.25
Test: [0/22]	Time 0.929 (0.929)	Precision 55.75 (55.75)	Recall 67.63 (67.63) 	 P_C 29.18 	 R_C 33.44 	 F_C 29.83 	 P_O 55.75 	 R_O 67.63 	 F_O 61.12
Test: [11/50]	  P_C 37.36 	 R_C 50.09 	 F_C 40.01 	 P_O 52.34 	 R_O 67.92 	 F_O 59.12 	 mAP 45.56
Train: [0/156]	Time 1.359 (1.359)	Loss 2.60 (2.60)	mAP 31.91 (31.91)
Train: [100/156]	Time 0.345 (0.362)	Loss 2.74 (2.54)	mAP 28.88 (30.59)
Train: [12/50]	Time 0.358	Loss 2.53 	mAP 30.59
Test: [0/22]	Time 0.905 (0.905)	Precision 54.33 (54.33)	Recall 66.05 (66.05) 	 P_C 29.73 	 R_C 32.69 	 F_C 29.54 	 P_O 54.33 	 R_O 66.05 	 F_O 59.62
Test: [12/50]	  P_C 38.31 	 R_C 51.08 	 F_C 41.40 	 P_O 53.66 	 R_O 67.17 	 F_O 59.66 	 mAP 46.27
Train: [0/156]	Time 0.991 (0.991)	Loss 2.33 (2.33)	mAP 30.40 (30.40)
Train: [100/156]	Time 0.345 (0.356)	Loss 2.45 (2.52)	mAP 31.56 (31.11)
Train: [13/50]	Time 0.354	Loss 2.52 	mAP 30.92
Test: [0/22]	Time 0.912 (0.912)	Precision 58.50 (58.50)	Recall 69.74 (69.74) 	 P_C 29.65 	 R_C 33.76 	 F_C 30.33 	 P_O 58.50 	 R_O 69.74 	 F_O 63.63
Test: [13/50]	  P_C 40.97 	 R_C 50.45 	 F_C 42.75 	 P_O 54.03 	 R_O 67.94 	 F_O 60.19 	 mAP 46.11
Train: [0/156]	Time 1.021 (1.021)	Loss 2.40 (2.40)	mAP 31.08 (31.08)
Train: [100/156]	Time 0.347 (0.357)	Loss 2.21 (2.49)	mAP 35.26 (30.80)
Train: [14/50]	Time 0.355	Loss 2.51 	mAP 30.80
Test: [0/22]	Time 0.898 (0.898)	Precision 58.98 (58.98)	Recall 70.00 (70.00) 	 P_C 32.40 	 R_C 33.07 	 F_C 30.95 	 P_O 58.98 	 R_O 70.00 	 F_O 64.02
Test: [14/50]	  P_C 41.28 	 R_C 50.05 	 F_C 41.67 	 P_O 53.79 	 R_O 67.92 	 F_O 60.04 	 mAP 46.34
Train: [0/156]	Time 1.018 (1.018)	Loss 2.27 (2.27)	mAP 33.58 (33.58)
Train: [100/156]	Time 0.349 (0.358)	Loss 2.87 (2.51)	mAP 32.99 (31.20)
Train: [15/50]	Time 0.356	Loss 2.50 	mAP 30.99
Test: [0/22]	Time 0.895 (0.895)	Precision 60.95 (60.95)	Recall 67.37 (67.37) 	 P_C 30.78 	 R_C 32.13 	 F_C 30.04 	 P_O 60.95 	 R_O 67.37 	 F_O 64.00
Test: [15/50]	  P_C 41.28 	 R_C 48.30 	 F_C 42.38 	 P_O 56.72 	 R_O 65.66 	 F_O 60.86 	 mAP 46.94
Train: [0/156]	Time 1.187 (1.187)	Loss 2.21 (2.21)	mAP 30.44 (30.44)
Train: [100/156]	Time 0.344 (0.360)	Loss 2.31 (2.50)	mAP 29.28 (31.31)
Train: [16/50]	Time 0.357	Loss 2.49 	mAP 31.10
Test: [0/22]	Time 0.921 (0.921)	Precision 57.79 (57.79)	Recall 67.37 (67.37) 	 P_C 30.02 	 R_C 33.09 	 F_C 30.31 	 P_O 57.79 	 R_O 67.37 	 F_O 62.21
Test: [16/50]	  P_C 41.19 	 R_C 51.35 	 F_C 42.43 	 P_O 54.64 	 R_O 67.97 	 F_O 60.58 	 mAP 47.39
Train: [0/156]	Time 0.813 (0.813)	Loss 2.27 (2.27)	mAP 39.45 (39.45)
Train: [100/156]	Time 0.349 (0.353)	Loss 2.44 (2.47)	mAP 31.99 (31.32)
Train: [17/50]	Time 0.352	Loss 2.47 	mAP 31.36
Test: [0/22]	Time 0.909 (0.909)	Precision 57.53 (57.53)	Recall 67.37 (67.37) 	 P_C 30.05 	 R_C 34.26 	 F_C 30.55 	 P_O 57.53 	 R_O 67.37 	 F_O 62.06
Test: [17/50]	  P_C 41.96 	 R_C 50.76 	 F_C 44.22 	 P_O 56.33 	 R_O 67.21 	 F_O 61.29 	 mAP 46.85
Train: [0/156]	Time 0.813 (0.813)	Loss 2.45 (2.45)	mAP 33.30 (33.30)
Train: [100/156]	Time 0.345 (0.359)	Loss 2.36 (2.46)	mAP 29.49 (31.03)
Train: [18/50]	Time 0.357	Loss 2.46 	mAP 31.22
Test: [0/22]	Time 0.935 (0.935)	Precision 59.13 (59.13)	Recall 68.16 (68.16) 	 P_C 31.53 	 R_C 32.79 	 F_C 30.90 	 P_O 59.13 	 R_O 68.16 	 F_O 63.33
Test: [18/50]	  P_C 40.13 	 R_C 51.82 	 F_C 43.68 	 P_O 55.06 	 R_O 67.68 	 F_O 60.72 	 mAP 47.39
Train: [0/156]	Time 1.236 (1.236)	Loss 2.16 (2.16)	mAP 35.71 (35.71)
Train: [100/156]	Time 0.414 (0.362)	Loss 2.46 (2.45)	mAP 28.37 (31.86)
Train: [19/50]	Time 0.358	Loss 2.43 	mAP 31.76
Test: [0/22]	Time 0.897 (0.897)	Precision 61.41 (61.41)	Recall 68.68 (68.68) 	 P_C 31.61 	 R_C 32.80 	 F_C 30.71 	 P_O 61.41 	 R_O 68.68 	 F_O 64.84
Test: [19/50]	  P_C 42.45 	 R_C 51.43 	 F_C 44.59 	 P_O 57.32 	 R_O 67.61 	 F_O 62.04 	 mAP 47.43
Train: [0/156]	Time 0.972 (0.972)	Loss 2.42 (2.42)	mAP 33.52 (33.52)
Train: [100/156]	Time 0.350 (0.356)	Loss 2.41 (2.43)	mAP 28.71 (31.71)
Train: [20/50]	Time 0.353	Loss 2.43 	mAP 31.50
Test: [0/22]	Time 0.905 (0.905)	Precision 59.29 (59.29)	Recall 70.53 (70.53) 	 P_C 32.69 	 R_C 34.98 	 F_C 31.92 	 P_O 59.29 	 R_O 70.53 	 F_O 64.42
Test: [20/50]	  P_C 42.11 	 R_C 50.38 	 F_C 43.51 	 P_O 55.41 	 R_O 68.56 	 F_O 61.29 	 mAP 47.45
Train: [0/156]	Time 1.288 (1.288)	Loss 2.71 (2.71)	mAP 33.52 (33.52)
Train: [100/156]	Time 0.350 (0.360)	Loss 2.25 (2.44)	mAP 31.28 (31.56)
Train: [21/50]	Time 0.358	Loss 2.42 	mAP 31.48
Test: [0/22]	Time 0.916 (0.916)	Precision 57.94 (57.94)	Recall 68.16 (68.16) 	 P_C 32.18 	 R_C 33.52 	 F_C 31.40 	 P_O 57.94 	 R_O 68.16 	 F_O 62.64
Test: [21/50]	  P_C 41.19 	 R_C 50.46 	 F_C 43.82 	 P_O 55.75 	 R_O 68.43 	 F_O 61.44 	 mAP 47.69
Train: [0/156]	Time 0.953 (0.953)	Loss 2.49 (2.49)	mAP 29.36 (29.36)
Train: [100/156]	Time 0.347 (0.357)	Loss 2.09 (2.38)	mAP 33.18 (32.13)
Train: [22/50]	Time 0.355	Loss 2.39 	mAP 32.26
Test: [0/22]	Time 0.925 (0.925)	Precision 60.05 (60.05)	Recall 68.42 (68.42) 	 P_C 32.14 	 R_C 33.66 	 F_C 31.31 	 P_O 60.05 	 R_O 68.42 	 F_O 63.96
Test: [22/50]	  P_C 42.23 	 R_C 51.11 	 F_C 44.54 	 P_O 57.06 	 R_O 68.09 	 F_O 62.09 	 mAP 48.38
Train: [0/156]	Time 1.182 (1.182)	Loss 2.28 (2.28)	mAP 31.44 (31.44)
Train: [100/156]	Time 0.346 (0.359)	Loss 2.58 (2.39)	mAP 30.50 (31.62)
Train: [23/50]	Time 0.356	Loss 2.40 	mAP 31.69
Test: [0/22]	Time 0.919 (0.919)	Precision 60.63 (60.63)	Recall 70.53 (70.53) 	 P_C 33.99 	 R_C 35.48 	 F_C 33.26 	 P_O 60.63 	 R_O 70.53 	 F_O 65.21
Test: [23/50]	  P_C 41.93 	 R_C 51.00 	 F_C 44.64 	 P_O 56.90 	 R_O 69.18 	 F_O 62.44 	 mAP 48.15
Train: [0/156]	Time 1.054 (1.054)	Loss 2.53 (2.53)	mAP 31.68 (31.68)
Train: [100/156]	Time 0.354 (0.361)	Loss 2.09 (2.35)	mAP 32.01 (31.92)
Train: [24/50]	Time 0.357	Loss 2.37 	mAP 32.26
Test: [0/22]	Time 0.913 (0.913)	Precision 61.83 (61.83)	Recall 69.47 (69.47) 	 P_C 34.83 	 R_C 36.23 	 F_C 33.94 	 P_O 61.83 	 R_O 69.47 	 F_O 65.43
Test: [24/50]	  P_C 42.88 	 R_C 53.07 	 F_C 45.80 	 P_O 57.24 	 R_O 68.22 	 F_O 62.25 	 mAP 48.59
Train: [0/156]	Time 0.934 (0.934)	Loss 2.08 (2.08)	mAP 33.95 (33.95)
Train: [100/156]	Time 0.352 (0.359)	Loss 2.61 (2.36)	mAP 30.15 (32.18)
Train: [25/50]	Time 0.357	Loss 2.38 	mAP 32.12
Test: [0/22]	Time 0.973 (0.973)	Precision 62.33 (62.33)	Recall 71.84 (71.84) 	 P_C 33.71 	 R_C 37.45 	 F_C 34.24 	 P_O 62.33 	 R_O 71.84 	 F_O 66.75
Test: [25/50]	  P_C 42.77 	 R_C 52.14 	 F_C 45.50 	 P_O 57.14 	 R_O 68.60 	 F_O 62.35 	 mAP 48.79
Train: [0/156]	Time 1.379 (1.379)	Loss 2.31 (2.31)	mAP 31.26 (31.26)
Train: [100/156]	Time 0.349 (0.364)	Loss 2.12 (2.37)	mAP 29.12 (32.26)
Train: [26/50]	Time 0.363	Loss 2.38 	mAP 32.22
Test: [0/22]	Time 0.916 (0.916)	Precision 58.42 (58.42)	Recall 70.26 (70.26) 	 P_C 32.80 	 R_C 35.50 	 F_C 32.59 	 P_O 58.42 	 R_O 70.26 	 F_O 63.80
Test: [26/50]	  P_C 42.00 	 R_C 52.62 	 F_C 45.27 	 P_O 56.06 	 R_O 69.27 	 F_O 61.97 	 mAP 48.72
Train: [0/156]	Time 1.127 (1.127)	Loss 2.35 (2.35)	mAP 33.66 (33.66)
Train: [100/156]	Time 0.348 (0.367)	Loss 2.62 (2.37)	mAP 30.52 (32.22)
Train: [27/50]	Time 0.363	Loss 2.38 	mAP 31.84
Test: [0/22]	Time 0.949 (0.949)	Precision 60.84 (60.84)	Recall 68.68 (68.68) 	 P_C 32.52 	 R_C 35.16 	 F_C 32.21 	 P_O 60.84 	 R_O 68.68 	 F_O 64.52
Test: [27/50]	  P_C 42.45 	 R_C 51.50 	 F_C 45.06 	 P_O 57.21 	 R_O 68.69 	 F_O 62.43 	 mAP 48.68
Train: [0/156]	Time 1.134 (1.134)	Loss 2.54 (2.54)	mAP 36.53 (36.53)
Train: [100/156]	Time 0.352 (0.363)	Loss 2.34 (2.33)	mAP 32.49 (32.18)
Train: [28/50]	Time 0.360	Loss 2.34 	mAP 32.37
Test: [0/22]	Time 0.996 (0.996)	Precision 58.84 (58.84)	Recall 69.21 (69.21) 	 P_C 31.57 	 R_C 35.08 	 F_C 31.49 	 P_O 58.84 	 R_O 69.21 	 F_O 63.60
Test: [28/50]	  P_C 42.11 	 R_C 51.69 	 F_C 45.15 	 P_O 56.67 	 R_O 69.18 	 F_O 62.31 	 mAP 48.46
Train: [0/156]	Time 1.210 (1.210)	Loss 2.45 (2.45)	mAP 31.32 (31.32)
Train: [100/156]	Time 0.345 (0.361)	Loss 2.30 (2.31)	mAP 31.78 (32.81)
Train: [29/50]	Time 0.357	Loss 2.31 	mAP 32.67
Test: [0/22]	Time 0.906 (0.906)	Precision 61.63 (61.63)	Recall 71.84 (71.84) 	 P_C 33.89 	 R_C 35.82 	 F_C 33.56 	 P_O 61.63 	 R_O 71.84 	 F_O 66.34
Test: [29/50]	  P_C 43.05 	 R_C 51.55 	 F_C 45.10 	 P_O 57.70 	 R_O 68.81 	 F_O 62.77 	 mAP 49.03
Train: [0/156]	Time 0.989 (0.989)	Loss 1.89 (1.89)	mAP 36.61 (36.61)
Train: [100/156]	Time 0.346 (0.357)	Loss 2.24 (2.31)	mAP 33.31 (32.63)
Train: [30/50]	Time 0.354	Loss 2.33 	mAP 32.42
Test: [0/22]	Time 0.908 (0.908)	Precision 61.54 (61.54)	Recall 69.47 (69.47) 	 P_C 32.41 	 R_C 34.05 	 F_C 32.01 	 P_O 61.54 	 R_O 69.47 	 F_O 65.27
Test: [30/50]	  P_C 43.06 	 R_C 50.85 	 F_C 45.19 	 P_O 58.83 	 R_O 68.53 	 F_O 63.31 	 mAP 48.68
Train: [0/156]	Time 0.939 (0.939)	Loss 2.12 (2.12)	mAP 33.33 (33.33)
Train: [100/156]	Time 0.410 (0.356)	Loss 2.21 (2.31)	mAP 33.39 (32.54)
Train: [31/50]	Time 0.355	Loss 2.32 	mAP 32.59
Test: [0/22]	Time 0.934 (0.934)	Precision 58.30 (58.30)	Recall 70.26 (70.26) 	 P_C 31.10 	 R_C 35.31 	 F_C 31.76 	 P_O 58.30 	 R_O 70.26 	 F_O 63.72
Test: [31/50]	  P_C 43.10 	 R_C 52.13 	 F_C 45.46 	 P_O 57.02 	 R_O 69.53 	 F_O 62.66 	 mAP 49.00
Train: [0/156]	Time 0.970 (0.970)	Loss 2.42 (2.42)	mAP 35.77 (35.77)
Train: [100/156]	Time 0.350 (0.363)	Loss 2.23 (2.29)	mAP 31.44 (32.52)
Train: [32/50]	Time 0.359	Loss 2.30 	mAP 32.64
Test: [0/22]	Time 0.929 (0.929)	Precision 62.59 (62.59)	Recall 71.32 (71.32) 	 P_C 33.39 	 R_C 35.45 	 F_C 33.28 	 P_O 62.59 	 R_O 71.32 	 F_O 66.67
Test: [32/50]	  P_C 42.41 	 R_C 52.78 	 F_C 45.15 	 P_O 57.52 	 R_O 69.13 	 F_O 62.79 	 mAP 48.98
Train: [0/156]	Time 1.411 (1.411)	Loss 2.29 (2.29)	mAP 38.88 (38.88)
Train: [100/156]	Time 0.353 (0.363)	Loss 2.10 (2.31)	mAP 30.97 (32.58)
Train: [33/50]	Time 0.359	Loss 2.32 	mAP 32.42
Test: [0/22]	Time 0.909 (0.909)	Precision 62.44 (62.44)	Recall 70.00 (70.00) 	 P_C 33.57 	 R_C 34.67 	 F_C 33.03 	 P_O 62.44 	 R_O 70.00 	 F_O 66.00
Test: [33/50]	  P_C 43.56 	 R_C 51.10 	 F_C 45.76 	 P_O 59.03 	 R_O 68.51 	 F_O 63.42 	 mAP 49.08
Train: [0/156]	Time 0.883 (0.883)	Loss 2.27 (2.27)	mAP 33.52 (33.52)
Train: [100/156]	Time 0.356 (0.356)	Loss 1.97 (2.28)	mAP 35.20 (32.63)
Train: [34/50]	Time 0.355	Loss 2.28 	mAP 32.76
Test: [0/22]	Time 0.921 (0.921)	Precision 63.29 (63.29)	Recall 68.95 (68.95) 	 P_C 33.65 	 R_C 34.30 	 F_C 32.76 	 P_O 63.29 	 R_O 68.95 	 F_O 65.99
Test: [34/50]	  P_C 43.56 	 R_C 51.92 	 F_C 45.82 	 P_O 59.04 	 R_O 68.27 	 F_O 63.32 	 mAP 49.20
Train: [0/156]	Time 0.821 (0.821)	Loss 2.21 (2.21)	mAP 33.12 (33.12)
Train: [100/156]	Time 0.344 (0.357)	Loss 2.31 (2.29)	mAP 31.62 (32.92)
Train: [35/50]	Time 0.354	Loss 2.30 	mAP 32.79
Test: [0/22]	Time 0.934 (0.934)	Precision 62.70 (62.70)	Recall 70.79 (70.79) 	 P_C 33.49 	 R_C 34.96 	 F_C 32.86 	 P_O 62.70 	 R_O 70.79 	 F_O 66.50
Test: [35/50]	  P_C 43.48 	 R_C 50.57 	 F_C 45.42 	 P_O 59.19 	 R_O 68.61 	 F_O 63.55 	 mAP 49.12
Train: [0/156]	Time 0.860 (0.860)	Loss 2.30 (2.30)	mAP 34.46 (34.46)
Train: [100/156]	Time 0.355 (0.355)	Loss 1.86 (2.29)	mAP 30.18 (32.79)
Train: [36/50]	Time 0.354	Loss 2.28 	mAP 32.84
Test: [0/22]	Time 0.918 (0.918)	Precision 61.98 (61.98)	Recall 70.79 (70.79) 	 P_C 35.13 	 R_C 36.43 	 F_C 34.40 	 P_O 61.98 	 R_O 70.79 	 F_O 66.09
Test: [36/50]	  P_C 44.30 	 R_C 52.03 	 F_C 46.47 	 P_O 58.71 	 R_O 69.07 	 F_O 63.47 	 mAP 49.41
Train: [0/156]	Time 0.886 (0.886)	Loss 2.21 (2.21)	mAP 30.11 (30.11)
Train: [100/156]	Time 0.350 (0.360)	Loss 2.03 (2.29)	mAP 31.75 (33.15)
Train: [37/50]	Time 0.356	Loss 2.28 	mAP 32.96
Test: [0/22]	Time 0.894 (0.894)	Precision 61.57 (61.57)	Recall 70.00 (70.00) 	 P_C 33.77 	 R_C 35.34 	 F_C 33.24 	 P_O 61.57 	 R_O 70.00 	 F_O 65.52
Test: [37/50]	  P_C 43.63 	 R_C 51.69 	 F_C 46.04 	 P_O 59.27 	 R_O 69.01 	 F_O 63.77 	 mAP 49.61
Train: [0/156]	Time 0.862 (0.862)	Loss 2.19 (2.19)	mAP 34.21 (34.21)
Train: [100/156]	Time 0.347 (0.354)	Loss 1.91 (2.23)	mAP 39.57 (32.93)
Train: [38/50]	Time 0.353	Loss 2.24 	mAP 33.17
Test: [0/22]	Time 0.906 (0.906)	Precision 60.87 (60.87)	Recall 70.00 (70.00) 	 P_C 32.67 	 R_C 34.63 	 F_C 32.45 	 P_O 60.87 	 R_O 70.00 	 F_O 65.12
Test: [38/50]	  P_C 43.86 	 R_C 51.91 	 F_C 45.97 	 P_O 57.87 	 R_O 69.38 	 F_O 63.11 	 mAP 49.46
Train: [0/156]	Time 1.041 (1.041)	Loss 2.36 (2.36)	mAP 33.54 (33.54)
Train: [100/156]	Time 0.346 (0.357)	Loss 2.62 (2.26)	mAP 35.95 (33.29)
Train: [39/50]	Time 0.355	Loss 2.26 	mAP 32.80
Test: [0/22]	Time 0.906 (0.906)	Precision 61.98 (61.98)	Recall 70.79 (70.79) 	 P_C 32.81 	 R_C 35.30 	 F_C 32.91 	 P_O 61.98 	 R_O 70.79 	 F_O 66.09
Test: [39/50]	  P_C 43.70 	 R_C 52.05 	 F_C 46.13 	 P_O 58.98 	 R_O 69.10 	 F_O 63.64 	 mAP 49.74
Train: [0/156]	Time 1.035 (1.035)	Loss 2.28 (2.28)	mAP 33.52 (33.52)
Train: [100/156]	Time 0.352 (0.361)	Loss 2.89 (2.26)	mAP 36.73 (33.25)
Train: [40/50]	Time 0.357	Loss 2.26 	mAP 32.93
Test: [0/22]	Time 0.924 (0.924)	Precision 61.81 (61.81)	Recall 70.26 (70.26) 	 P_C 33.78 	 R_C 36.13 	 F_C 33.63 	 P_O 61.81 	 R_O 70.26 	 F_O 65.76
Test: [40/50]	  P_C 44.79 	 R_C 51.48 	 F_C 46.56 	 P_O 59.38 	 R_O 68.97 	 F_O 63.82 	 mAP 49.45
Train: [0/156]	Time 0.986 (0.986)	Loss 1.98 (1.98)	mAP 32.40 (32.40)
Train: [100/156]	Time 0.351 (0.361)	Loss 2.04 (2.26)	mAP 33.67 (33.29)
Train: [41/50]	Time 0.358	Loss 2.25 	mAP 32.95
Test: [0/22]	Time 0.945 (0.945)	Precision 61.75 (61.75)	Recall 70.53 (70.53) 	 P_C 34.26 	 R_C 36.04 	 F_C 33.84 	 P_O 61.75 	 R_O 70.53 	 F_O 65.85
Test: [41/50]	  P_C 43.50 	 R_C 51.34 	 F_C 45.95 	 P_O 59.25 	 R_O 68.85 	 F_O 63.69 	 mAP 49.60
Train: [0/156]	Time 0.961 (0.961)	Loss 2.30 (2.30)	mAP 34.58 (34.58)
Train: [100/156]	Time 0.382 (0.360)	Loss 2.36 (2.22)	mAP 35.24 (32.99)
Train: [42/50]	Time 0.358	Loss 2.25 	mAP 33.16
Test: [0/22]	Time 0.930 (0.930)	Precision 62.44 (62.44)	Recall 70.00 (70.00) 	 P_C 34.26 	 R_C 35.82 	 F_C 33.78 	 P_O 62.44 	 R_O 70.00 	 F_O 66.00
Test: [42/50]	  P_C 44.81 	 R_C 51.45 	 F_C 46.51 	 P_O 60.21 	 R_O 68.47 	 F_O 64.08 	 mAP 49.67
Train: [0/156]	Time 0.798 (0.798)	Loss 1.95 (1.95)	mAP 31.61 (31.61)
Train: [100/156]	Time 0.347 (0.359)	Loss 2.29 (2.27)	mAP 26.67 (33.33)
Train: [43/50]	Time 0.356	Loss 2.27 	mAP 33.35
Test: [0/22]	Time 0.918 (0.918)	Precision 63.10 (63.10)	Recall 69.74 (69.74) 	 P_C 34.15 	 R_C 35.07 	 F_C 33.39 	 P_O 63.10 	 R_O 69.74 	 F_O 66.25
Test: [43/50]	  P_C 44.07 	 R_C 52.46 	 F_C 46.47 	 P_O 59.50 	 R_O 68.86 	 F_O 63.84 	 mAP 49.61
Train: [0/156]	Time 1.030 (1.030)	Loss 2.66 (2.66)	mAP 36.94 (36.94)
Train: [100/156]	Time 0.347 (0.358)	Loss 2.21 (2.26)	mAP 40.08 (32.67)
Train: [44/50]	Time 0.356	Loss 2.25 	mAP 32.84
Test: [0/22]	Time 0.918 (0.918)	Precision 61.98 (61.98)	Recall 70.79 (70.79) 	 P_C 34.13 	 R_C 35.82 	 F_C 33.69 	 P_O 61.98 	 R_O 70.79 	 F_O 66.09
Test: [44/50]	  P_C 44.75 	 R_C 52.30 	 F_C 46.84 	 P_O 59.41 	 R_O 69.10 	 F_O 63.89 	 mAP 49.68
Train: [0/156]	Time 1.164 (1.164)	Loss 2.06 (2.06)	mAP 34.65 (34.65)
Train: [100/156]	Time 0.350 (0.361)	Loss 2.01 (2.24)	mAP 28.19 (33.26)
Train: [45/50]	Time 0.358	Loss 2.24 	mAP 33.23
Test: [0/22]	Time 0.915 (0.915)	Precision 62.79 (62.79)	Recall 71.05 (71.05) 	 P_C 33.95 	 R_C 35.43 	 F_C 33.49 	 P_O 62.79 	 R_O 71.05 	 F_O 66.67
Test: [45/50]	  P_C 44.91 	 R_C 52.46 	 F_C 46.88 	 P_O 59.70 	 R_O 68.92 	 F_O 63.98 	 mAP 49.71
Train: [0/156]	Time 1.060 (1.060)	Loss 1.97 (1.97)	mAP 31.55 (31.55)
Train: [100/156]	Time 0.353 (0.357)	Loss 2.27 (2.23)	mAP 31.66 (33.67)
Train: [46/50]	Time 0.354	Loss 2.22 	mAP 33.52
Test: [0/22]	Time 0.918 (0.918)	Precision 62.00 (62.00)	Recall 70.00 (70.00) 	 P_C 33.55 	 R_C 34.62 	 F_C 32.90 	 P_O 62.00 	 R_O 70.00 	 F_O 65.76
Test: [46/50]	  P_C 43.66 	 R_C 52.16 	 F_C 46.49 	 P_O 59.84 	 R_O 69.08 	 F_O 64.13 	 mAP 49.71
Train: [0/156]	Time 1.092 (1.092)	Loss 2.18 (2.18)	mAP 38.59 (38.59)
Train: [100/156]	Time 0.347 (0.358)	Loss 2.51 (2.24)	mAP 31.72 (32.99)
Train: [47/50]	Time 0.356	Loss 2.24 	mAP 33.04
Test: [0/22]	Time 0.911 (0.911)	Precision 63.00 (63.00)	Recall 70.79 (70.79) 	 P_C 33.72 	 R_C 34.95 	 F_C 33.17 	 P_O 63.00 	 R_O 70.79 	 F_O 66.67
Test: [47/50]	  P_C 44.46 	 R_C 52.67 	 F_C 46.93 	 P_O 59.70 	 R_O 69.17 	 F_O 64.09 	 mAP 49.67
Train: [0/156]	Time 1.019 (1.019)	Loss 2.27 (2.27)	mAP 37.28 (37.28)
Train: [100/156]	Time 0.345 (0.361)	Loss 1.84 (2.20)	mAP 27.56 (33.51)
Train: [48/50]	Time 0.357	Loss 2.20 	mAP 33.37
Test: [0/22]	Time 0.919 (0.919)	Precision 62.68 (62.68)	Recall 70.26 (70.26) 	 P_C 33.83 	 R_C 35.16 	 F_C 33.30 	 P_O 62.68 	 R_O 70.26 	 F_O 66.25
Test: [48/50]	  P_C 44.68 	 R_C 52.44 	 F_C 46.99 	 P_O 59.84 	 R_O 69.14 	 F_O 64.16 	 mAP 49.71
Train: [0/156]	Time 1.073 (1.073)	Loss 2.42 (2.42)	mAP 34.91 (34.91)
Train: [100/156]	Time 0.349 (0.357)	Loss 2.12 (2.23)	mAP 37.39 (33.60)
Train: [49/50]	Time 0.355	Loss 2.23 	mAP 33.27
Test: [0/22]	Time 0.912 (0.912)	Precision 62.09 (62.09)	Recall 70.26 (70.26) 	 P_C 33.67 	 R_C 35.12 	 F_C 33.20 	 P_O 62.09 	 R_O 70.26 	 F_O 65.93
Test: [49/50]	  P_C 44.62 	 R_C 52.41 	 F_C 46.95 	 P_O 59.69 	 R_O 69.13 	 F_O 64.06 	 mAP 49.72
Train: [0/156]	Time 0.857 (0.857)	Loss 2.50 (2.50)	mAP 30.64 (30.64)
Train: [100/156]	Time 0.345 (0.354)	Loss 2.37 (2.25)	mAP 30.52 (33.63)
Train: [50/50]	Time 0.353	Loss 2.22 	mAP 33.37
Test: [0/22]	Time 0.910 (0.910)	Precision 62.30 (62.30)	Recall 70.00 (70.00) 	 P_C 33.71 	 R_C 35.02 	 F_C 33.18 	 P_O 62.30 	 R_O 70.00 	 F_O 65.92
Test: [50/50]	  P_C 44.63 	 R_C 52.32 	 F_C 46.89 	 P_O 59.70 	 R_O 69.07 	 F_O 64.04 	 mAP 49.73
Evaluating the best model
Evaluate with threshold 0.50
... loading pretrained weights from ./output/foodseg103-RN101-cosine-bs32-e50/model_best.pth.tar
Test: [0/22]	Time 0.910 (0.910)	Precision 61.98 (61.98)	Recall 70.79 (70.79) 	 P_C 32.81 	 R_C 35.30 	 F_C 32.91 	 P_O 61.98 	 R_O 70.79 	 F_O 66.09
Test: [39/50]	  P_C 43.70 	 R_C 52.05 	 F_C 46.13 	 P_O 58.98 	 R_O 69.10 	 F_O 63.64 	 mAP 49.74
